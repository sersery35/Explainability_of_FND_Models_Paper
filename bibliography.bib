@article{ThePsycologyOfFakeNews_Pennycook,
  title    = {The Psychology of Fake News},
  journal  = {Trends in Cognitive Sciences},
  volume   = {25},
  number   = {5},
  pages    = {388-402},
  year     = {2021},
  issn     = {1364-6613},
  doi      = {https://doi.org/10.1016/j.tics.2021.02.007},
  url      = {https://www.sciencedirect.com/science/article/pii/S1364661321000516},
  author   = {Gordon Pennycook and David G. Rand},
  keywords = {fake news, misinformation, social media, news media, motivated reasoning, dual process theory, crowdsourcing, attention, information sharing},
  abstract = {We synthesize a burgeoning literature investigating why people believe and share false or highly misleading news online. Contrary to a common narrative whereby politics drives susceptibility to fake news, people are ‘better’ at discerning truth from falsehood (despite greater overall belief) when evaluating politically concordant news. Instead, poor truth discernment is associated with lack of careful reasoning and relevant knowledge, and the use of heuristics such as familiarity. Furthermore, there is a substantial disconnect between what people believe and what they share on social media. This dissociation is largely driven by inattention, more so than by purposeful sharing of misinformation. Thus, interventions can successfully nudge social media users to focus more on accuracy. Crowdsourced veracity ratings can also be leveraged to improve social media ranking algorithms.}
}

@article{TheScienceOfFakeNews_Lazer,
  author   = {David M. J. Lazer  and Matthew A. Baum  and Yochai Benkler  and Adam J. Berinsky  and Kelly M. Greenhill  and Filippo Menczer  and Miriam J. Metzger  and Brendan Nyhan  and Gordon Pennycook  and David Rothschild  and Michael Schudson  and Steven A. Sloman  and Cass R. Sunstein  and Emily A. Thorson  and Duncan J. Watts  and Jonathan L. Zittrain },
  title    = {The science of fake news},
  journal  = {Science},
  volume   = {359},
  number   = {6380},
  pages    = {1094-1096},
  year     = {2018},
  doi      = {10.1126/science.aao2998},
  url      = {https://www.science.org/doi/abs/10.1126/science.aao2998},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.aao2998},
  abstract = {Addressing fake news requires a multidisciplinary effort The rise of fake news highlights the erosion of long-standing institutional bulwarks against misinformation in the internet age. Concern over the problem is global. However, much remains unknown regarding the vulnerabilities of individuals, institutions, and society to manipulations by malicious actors. A new system of safeguards is needed. Below, we discuss extant social and computer science research regarding belief in fake news and the mechanisms by which it spreads. Fake news has a long history, but we focus on unanswered scientific questions raised by the proliferation of its most recent, politically oriented incarnation. Beyond selected references in the text, suggested further reading can be found in the supplementary materials.}
}

@article{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu,
  author     = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  title      = {Fake News Detection on Social Media: A Data Mining Perspective},
  year       = {2017},
  issue_date = {June 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {19},
  number     = {1},
  issn       = {1931-0145},
  url        = {https://doi.org/10.1145/3137597.3137600},
  doi        = {10.1145/3137597.3137600},
  abstract   = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
  journal    = {SIGKDD Explor. Newsl.},
  month      = {sep},
  pages      = {22–36},
  numpages   = {15}
}

@article{FakeNewsAndThePublic_McKernon,
  title    = {Fake news and the public. How the press combats rumor, the market rigger, and the propagandist},
  issn     = {0017-789X},
  url      = {https://harpers.org/archive/1925/10/fake-news-and-the-public/},
  urldate  = {2018-05-04},
  journal  = {Harper's Magazine},
  author   = {McKernon, Edward},
  month    = oct,
  year     = {1925},
  keywords = {20th century, Journalism, Journalistic ethics, News agencies, Press and propaganda, Rumor, Stock exchanges and current events, United States}
}

 @article{ReutersInstituteDigitalNewsReport,
  title     = {Reuters Institute Digital News Report 2022},
  url       = {https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2022-06/Digital_News-Report_2022.pdf},
  journal   = {Digital News Report 2022},
  publisher = {Reuters Institute for the Study of Journalism},
  author    = {Newman, Nic and Fletcher, Richard and Robertson, Craig T. and Eddy, Kirsten and Nielsen, Rasmus Kleis},
  year      = {2022},
  month     = {06}
}

@inbook{USPresidentialElection2016,
  title     = {United States Presidential Election of 2016},
  booktitle = {Encyclopedia Britannica},
  author    = {Beckwith, David C},
  abstract  = {United States Presidential Election of 2016, American
               presidential election held on November 8, 2016, in which
               Republican Donald Trump lost the popular vote to Democrat
               Hillary Clinton by more than 2.8 million votes but won 30 states
               and the decisive electoral college with 304 electoral votes to
               Clinton's 227 and thus became the 45th president of the United
               States. The tumultuous, abrasive 2016 campaign defied
               established political norms. Clinton's campaign featured
               superior organization and fund-raising---and almost every
               election-eve poll pointed to a comfortable victory for her---but
               Trump's anti-Washington appeal to white working-class voters
               outside major cities in pivotal manufacturing states},
  month     = {10},
  year      = {2021}
}

@misc{FakeNewsNet_Shu,
  doi       = {10.48550/ARXIV.1809.01286},
  url       = {https://arxiv.org/abs/1809.01286},
  author    = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  keywords  = {Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {FakeNewsNet: A Data Repository with News Content, Social Context and Spatialtemporal Information for Studying Fake News on Social Media},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{LiarLiarPantsOnFire_Wang,
  author     = {William Yang Wang},
  title      = {"Liar, Liar Pants on Fire": {A} New Benchmark Dataset for Fake News
                Detection},
  journal    = {CoRR},
  volume     = {abs/1705.00648},
  year       = {2017},
  url        = {http://arxiv.org/abs/1705.00648},
  eprinttype = {arXiv},
  eprint     = {1705.00648},
  timestamp  = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Wang17j.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{FakeNewsDetectionUsingGeometricDeepLearning_Monti,
  author     = {Federico Monti and
                Fabrizio Frasca and
                Davide Eynard and
                Damon Mannion and
                Michael M. Bronstein},
  title      = {Fake News Detection on Social Media using Geometric Deep Learning},
  journal    = {CoRR},
  volume     = {abs/1902.06673},
  year       = {2019},
  url        = {http://arxiv.org/abs/1902.06673},
  eprinttype = {arXiv},
  eprint     = {1902.06673},
  timestamp  = {Tue, 21 May 2019 18:03:39 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1902-06673.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{UPFD_Dataset_Shu,
  author     = {Yingtong Dou and
                Kai Shu and
                Congying Xia and
                Philip S. Yu and
                Lichao Sun},
  title      = {User Preference-aware Fake News Detection},
  journal    = {CoRR},
  volume     = {abs/2104.12259},
  year       = {2021},
  url        = {https://arxiv.org/abs/2104.12259},
  eprinttype = {arXiv},
  eprint     = {2104.12259},
  timestamp  = {Mon, 03 May 2021 17:38:30 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-12259.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{GraphAttentionNetworks_Velickovic,
  doi       = {10.48550/ARXIV.1710.10903},
  url       = {https://arxiv.org/abs/1710.10903},
  author    = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  keywords  = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Graph Attention Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{FakeReddit_Nakamura,
  title     = {{F}akeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection},
  author    = {Nakamura, Kai  and
               Levy, Sharon  and
               Wang, William Yang},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.755},
  pages     = {6149--6157},
  abstract  = {Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}

@article{SomeLikeItHoaxDataset_Tacchini,
  doi       = {10.48550/ARXIV.1704.07506},
  url       = {https://arxiv.org/abs/1704.07506},
  author    = {Tacchini, Eugenio and Ballarin, Gabriele and Della Vedova, Marco L. and Moret, Stefano and de Alfaro, Luca},
  keywords  = {Machine Learning (cs.LG), Human-Computer Interaction (cs.HC), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Some Like it Hoax: Automated Fake News Detection in Social Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@paper{BuzzfaceDataset_Santia,
  author     = {Giovanni Santia and Jake Williams},
  title      = {BuzzFace: A News Veracity Dataset with Facebook User Commentary and Egos},
  conference = {International AAAI Conference on Web and Social Media},
  year       = {2018},
  keywords   = {Facebook; news veracity; Disqus; social bots},
  abstract   = {Veracity assessment of news and social bot detection have become two of the most pressing issues for social media platforms, yet current gold-standard data are limited. This paper presents a leap forward in the development of a sizeable and feature rich gold-standard dataset. The dataset was built by using a collection of news items posted to Facebook by nine news outlets during September 2016, which were annotated for veracity by BuzzFeed. These articles were refined beyond binary annotation to the four categories: mostly true, mostly false, mixture of true and false, and no factual content. Our contribution integrates data on Facebook comments and reactions publicly available on the platform’s Graph API, and provides tailored tools for accessing news article web content. The features of the accessed articles include body text, images, links, Facebook plugin comments, Disqus plugin comments, and embedded tweets. Embedded tweets provide a potent possible avenue for expansion across social media platforms. Upon development, this utility yielded over 1.6 million text items, making it over 400 times larger than the current gold-standard. The resulting dataset—BuzzFace—is presently the most extensive created, and allows for more robust machine learning applications to news veracity assessment and social bot detection than ever before.},
  url        = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17825/17046}
}

 @misc{NewsConsumptionAcrossSocialMedia_pewresearch,
  title     = {News consumption across social media in 2021},
  url       = {https://www.pewresearch.org/journalism/2021/09/20/news-consumption-across-social-media-in-2021/},
  journal   = {Pew Research Center's Journalism Project},
  publisher = {Pew Research Center},
  author    = {Walker, Mason and Matsa, Katerina Eva},
  year      = {2021},
  month     = {Sep}
}

@article{GetBackYouDontKnowMeLikeThat_Hannak,
  title        = {Get Back! You Don’t Know Me Like That: The Social Mediation of Fact Checking Interventions in Twitter Conversations},
  volume       = {8},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/14555},
  abstractnote = { &lt;p&gt; &lt;span&gt;The prevalence of misinformation within social media and online communities can undermine public security and distract attention from important issues. Fact-checking interventions, in which users cite fact-checking websites such as Snopes.com and FactCheck.org, are a strategy users can employ to refute false claims made by their peers. While laboratory research suggests such interventions are not effective in persuading people to abandon false ideas, little work considers how such interventions are actually deployed in real-world conversations. Using approximately 1,600 interventions observed on Twitter between 2012 and 2013, we examine the contexts and consequences of fact-checking interventions.W&lt;/span&gt;&lt;span&gt;e focus in particular on the social relationship between the individual who issues the fact-check and the individual whose facts are challenged. Our results indicate that though fact-checking interventions are most commonly issued by strangers, they are more likely to draw user attention and responses when they come from friends. Finally, we discuss implications for designing more effective interventions against misinformation.&lt;/span&gt; &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Hannak, Aniko and Margolin, Drew and Keegan, Brian and Weber, Ingmar},
  year         = {2014},
  month        = {May},
  pages        = {187-196}
}

@misc{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han,
  doi       = {10.48550/ARXIV.2007.03316},
  url       = {https://arxiv.org/abs/2007.03316},
  author    = {Han, Yi and Karunasekera, Shanika and Leckie, Christopher},
  keywords  = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Graph Neural Networks with Continual Learning for Fake News Detection from Social Media},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{RumorDetectionBidirectionalGraphConvolutionalNetworks_Bian,
  doi       = {10.48550/ARXIV.2001.06362},
  url       = {https://arxiv.org/abs/2001.06362},
  author    = {Bian, Tian and Xiao, Xi and Xu, Tingyang and Zhao, Peilin and Huang, Wenbing and Rong, Yu and Huang, Junzhou},
  keywords  = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SAFEFND_Zhou,
  doi       = {10.48550/ARXIV.2003.04981},
  url       = {https://arxiv.org/abs/2003.04981},
  author    = {Zhou, Xinyi and Wu, Jindi and Zafarani, Reza},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {SAFE: Similarity-Aware Multi-Modal Fake News Detection},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{XAIConceptsTaxonomies_Arrieta,
  title    = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  journal  = {Information Fusion},
  volume   = {58},
  pages    = {82-115},
  year     = {2020},
  issn     = {1566-2535},
  doi      = {https://doi.org/10.1016/j.inffus.2019.12.012},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  author   = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}

@book{InterpretableMachineLearning_Molnar,
  title    = {Interpretable Machine Learning},
  author   = {Christoph Molnar},
  year     = {2022},
  subtitle = {A Guide for Making Black Box Models Explainable},
  edition  = {2},
  url      = {https://christophm.github.io/interpretable-ml-book}
}

@misc{TowardsARigorousScienceML_Velez,
  doi       = {10.48550/ARXIV.1702.08608},
  url       = {https://arxiv.org/abs/1702.08608},
  author    = {Doshi-Velez, Finale and Kim, Been},
  keywords  = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Towards A Rigorous Science of Interpretable Machine Learning},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{CanWeOpenTheBlackBoxOfAI_Castelvecchi,
  author  = {Castelvecchi, Davide},
  year    = {2016},
  month   = {10},
  pages   = {20-23},
  title   = {Can we open the black box of AI?},
  volume  = {538},
  journal = {Nature},
  doi     = {10.1038/538020a}
}

@article{XAI_Gunning,
  title        = {DARPA’s Explainable Artificial Intelligence (XAI) Program},
  volume       = {40},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2850},
  doi          = {10.1609/aimag.v40i2.2850},
  abstractnote = {&lt;p&gt;Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.&lt;/p&gt;},
  number       = {2},
  journal      = {AI Magazine},
  author       = {Gunning, David and Aha, David},
  year         = {2019},
  month        = {Jun.},
  pages        = {44-58}
}

@misc{TheMythosOfModelInterpretability_Lipton,
  doi       = {10.48550/ARXIV.1606.03490},
  url       = {https://arxiv.org/abs/1606.03490},
  author    = {Lipton, Zachary C.},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {The Mythos of Model Interpretability},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SlaveToTheAlgorithm_EdwardsVeale,
  title   = {Slave to the Algorithm? Why a 'Right to an Explanation' Is Probably Not the Remedy You Are Looking For},
  author  = {Lilian Edwards and Michael Veale},
  journal = {Duke law and technology review},
  year    = {2017},
  volume  = {16},
  pages   = {18-84}
}

@article{HierarchicalPropagationNetworksForFND_Shu,
  title        = {Hierarchical Propagation Networks for Fake News Detection: Investigation and Exploitation},
  volume       = {14},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/7329},
  abstractnote = {&lt;p&gt;Consuming news from social media is becoming increasingly popular. However, social media also enables the wide dissemination of &lt;em&gt;fake news&lt;/em&gt;. Because of the detrimental effects of fake news, fake news detection has attracted increasing attention. However, the performance of detecting fake news only from news content is generally limited as fake news pieces are written to mimic true news. In the real world, news pieces spread through &lt;em&gt;propagation networks&lt;/em&gt; on social media. The news propagation networks usually involve multi-levels. In this paper, we study the challenging problem of &lt;em&gt;investigating&lt;/em&gt; and &lt;em&gt;exploiting&lt;/em&gt; news hierarchical propagation network on social media for fake news detection.&lt;/p&gt;&lt;p&gt;In an attempt to understand the correlations between news propagation networks and fake news, first, we build hierarchical propagation networks for fake news and true news pieces; second, we perform a comparative analysis of the propagation network features from structural, temporal, and linguistic perspectives between fake and real news, which demonstrates the potential of utilizing these features to detect fake news; third, we show the effectiveness of these propagation network features for fake news detection. We further validate the effectiveness of these features from feature importance analysis. We conduct extensive experiments on real-world datasets and demonstrate the proposed features can significantly outperform state-of-the-art fake news detection methods by at least 1.7% with an average F1&gt;0.84. Altogether, this work presents a data-driven view of hierarchical propagation network and fake news and paves the way towards a healthier online news ecosystem.&lt;/p&gt;},
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Liu, Huan},
  year         = {2020},
  month        = {May},
  pages        = {626-637}
}

 @inbook{HistorysGreatestLies_Weir,
  place     = {Beverly, Massachusetts},
  booktitle = {History's greatest lies: The startling truths behind world events our history books got wrong},
  publisher = {Fair Winds Press},
  author    = {Weir, William},
  year      = {2009},
  pages     = {28–41}
}

 @misc{MarketQuaversAfterFakeAPTweet_ElBoghdady,
  title     = {Market quavers after fake AP tweet says Obama was hurt in White House explosions},
  url       = {https://www.washingtonpost.com/business/economy/market-quavers-after-fake-ap-tweet-says-obama-was-hurt-in-white-house-explosions/2013/04/23/d96d2dc6-ac4d-11e2-a8b9-2a63d75b5459_story.html},
  journal   = {The Washington Post},
  publisher = {WP Company},
  author    = {ElBoghdady, Dina},
  year      = {2013},
  month     = {Apr}
}

 @misc{Pizzagate_Fisher,
  title     = {Pizzagate: From rumor, to hashtag, to gunfire in D.C.},
  url       = {https://www.washingtonpost.com/local/pizzagate-from-rumor-to-hashtag-to-gunfire-in-dc/2016/12/06/4c7def50-bbd4-11e6-94ac-3d324840106c_story.html},
  journal   = {The Washington Post},
  publisher = {WP Company},
  author    = {Fisher, Marc and Cox, John Woodrow and Hermann, Peter},
  year      = {2016},
  month     = {Dec}
} 

 @misc{StatistaUsageOfSocialMedia_Watson,
  title   = {Usage of social media as a news source worldwide 2022},
  url     = {https://www.statista.com/statistics/718019/social-media-news-source/},
  journal = {Statista},
  author  = {Watson, Amy},
  year    = {2022},
  month   = {Aug}
}

@article{SocialMediaAndFakeNewsIn2016Election_Allcott,
  author  = {Allcott, Hunt and Gentzkow, Matthew},
  title   = {Social Media and Fake News in the 2016 Election},
  journal = {Journal of Economic Perspectives},
  volume  = {31},
  number  = {2},
  year    = {2017},
  month   = {May},
  pages   = {211-36},
  doi     = {10.1257/jep.31.2.211},
  url     = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.211}
}

@inbook{TheGreatMoonHoax_Foster,
  author    = {Foster, Vincent S.},
  title     = {The Great Moon Hoax},
  booktitle = {Modern Mysteries of the Moon: What We Still Don't Know About Our Lunar Companion},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {11--44},
  abstract  = {Back in 1835 there was a Moon hoax that thousands of people fell for, despite the tale being complete fiction. A series of articles were published in a newspaper reporting incredible new astronomical observations of the Moon supposedly made by astronomer Sir John Herschel during an observing run at the Cape of Good Hope. Detailed descriptions of winged beings, plants, animals and a sapphire temple increased sales and subscriptions to the fledgling newspaper. The chapter offers a selection of these articles as they were published in the New York Sun.},
  isbn      = {978-3-319-22120-5},
  doi       = {10.1007/978-3-319-22120-5_2},
  url       = {https://doi.org/10.1007/978-3-319-22120-5_2}
}

 @misc{Buzzfeed_FakeNewsOutperformRealNews_Silverman,
  title     = {This analysis shows how viral fake election news stories outperformed Real News on facebook},
  url       = {https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook},
  journal   = {BuzzFeed News},
  publisher = {BuzzFeed News},
  author    = {Silverman, Craig},
  year      = {2016},
  month     = {Nov}
} 

 @misc{TrumpWonBecauseOfFacebook_Read,
  title     = {Donald Trump won because of Facebook},
  url       = {https://nymag.com/intelligencer/2016/11/donald-trump-won-because-of-facebook.html},
  journal   = {Intelligencer},
  publisher = {Intelligencer},
  author    = {Read, Max},
  year      = {2016},
  month     = {Nov}
}

@article{ConfirmationBias_Nickerson,
  author   = {Raymond S. Nickerson},
  title    = {Confirmation Bias: A Ubiquitous Phenomenon in Many Guises},
  journal  = {Review of General Psychology},
  volume   = {2},
  number   = {2},
  pages    = {175-220},
  year     = {1998},
  doi      = {10.1037/1089-2680.2.2.175},
  url      = { 
              https://doi.org/10.1037/1089-2680.2.2.175
              
              },
  eprint   = { 
              https://doi.org/10.1037/1089-2680.2.2.175
              
              },
  abstract = { Confirmation bias, as the term is typically used in the psychological literature, connotes the seeking or interpreting of evidence in ways that are partial to existing beliefs, expectations, or a hypothesis in hand. The author reviews evidence of such a bias in a variety of guises and gives examples of its operation in several practical contexts. Possible explanations are considered, and the question of its utility or disutility is discussed. }
}

@inproceedings{NaiveRealism_Reed,
  title  = {Naive Realism in Everyday Life: Implications for Social Conflict and Misunderstanding},
  author = {Edward S. Reed and Elliot Turiel and Terrance Brown},
  year   = {2013}
}

@article{SocialIdentityTheory_Ashforth,
  issn      = {03637425},
  url       = {http://www.jstor.org/stable/258189},
  abstract  = {It is argued that (a) social identification is a perception of oneness with a group of persons; (b) social identification stems from the categorization of individuals, the distinctiveness and prestige of the group, the salience of outgroups, and the factors that traditionally are associated with group formation; and (c) social identification leads to activities that are congruent with the identity, support for institutions that embody the identity, stereotypical perceptions of self and others, and outcomes that traditionally are associated with group formation, and it reinforces the antecedents of identification. This perspective is applied to organizational socialization, role conflict, and intergroup relations.},
  author    = {Blake E. Ashforth and Fred Mael},
  journal   = {The Academy of Management Review},
  number    = {1},
  pages     = {20--39},
  publisher = {Academy of Management},
  title     = {Social Identity Theory and the Organization},
  urldate   = {2022-09-12},
  volume    = {14},
  year      = {1989}
}

@article{NormativeSocialInfluence_Asch,
  journal = {Groups, leadership, and men},
  title   = {Effects of group pressure upon the modification and distortion of
             judgments.},
  author  = {Asch, S E and Guetzkow, H},
  pages   = {222--236},
  year    = {1951}
}

 @book{TheFilterBubble_Pariser,
  place     = {London},
  title     = {The filter bubble: What the internet is hiding from you},
  publisher = {Penguin UK},
  author    = {Pariser, Eli},
  year      = {2011}
}

 @book{EchoChambers_Sunstein,
  place     = {Princeton, NJ},
  title     = {Echo chambers: Bush v. Gore, impeachment, and beyond},
  publisher = {Princeton University Press},
  author    = {Sunstein, Cass R.},
  year      = {2001}
} 

@article{TheSpreadingOfMisinformationOnline_DelVicario,
  author   = {Michela Del Vicario  and Alessandro Bessi  and Fabiana Zollo  and Fabio Petroni  and Antonio Scala  and Guido Caldarelli  and H. Eugene Stanley  and Walter Quattrociocchi },
  title    = {The spreading of misinformation online},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {113},
  number   = {3},
  pages    = {554-559},
  year     = {2016},
  doi      = {10.1073/pnas.1517441113},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1517441113},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1517441113},
  abstract = {The wide availability of user-provided content in online social media facilitates the aggregation of people around common interests, worldviews, and narratives. However, the World Wide Web (WWW) also allows for the rapid dissemination of unsubstantiated rumors and conspiracy theories that often elicit rapid, large, but naive social responses such as the recent case of Jade Helm 15––where a simple military exercise turned out to be perceived as the beginning of a new civil war in the United States. In this work, we address the determinants governing misinformation spreading through a thorough quantitative analysis. In particular, we focus on how Facebook users consume information related to two distinct narratives: scientific and conspiracy news. We find that, although consumers of scientific and conspiracy stories present similar consumption patterns with respect to content, cascade dynamics differ. Selective exposure to content is the primary driver of content diffusion and generates the formation of homogeneous clusters, i.e., “echo chambers.” Indeed, homogeneity appears to be the primary driver for the diffusion of contents and each echo chamber has its own cascade dynamics. Finally, we introduce a data-driven percolation model mimicking rumor spreading and we show that homogeneity and polarization are the main determinants for predicting cascades’ size.}
}

@article{AutomaticDeceptionDetection_Conroy,
  author   = {Conroy, Nadia K. and Rubin, Victoria L. and Chen, Yimin},
  title    = {Automatic deception detection: Methods for finding fake news},
  journal  = {Proceedings of the Association for Information Science and Technology},
  volume   = {52},
  number   = {1},
  pages    = {1-4},
  keywords = {Deception detection, fake news detection, veracity assessment, news verification, methods, automation, SVM, knowledge networks, predictive modelling, fraud},
  doi      = {https://doi.org/10.1002/pra2.2015.145052010082},
  url      = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.2015.145052010082},
  eprint   = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.2015.145052010082},
  abstract = {ABSTRACT This research surveys the current state-of-the-art technologies that are instrumental in the adoption and development of fake news detection. “Fake news detection” is defined as the task of categorizing news along a continuum of veracity, with an associated measure of certainty. Veracity is compromised by the occurrence of intentional deceptions. The nature of online news publication has changed, such that traditional fact checking and vetting from potential deception is impossible against the flood arising from content generators, as well as various formats and genres. The paper provides a typology of several varieties of veracity assessment methods emerging from two major categories – linguistic cue approaches (with machine learning), and network analysis approaches. We see promise in an innovative hybrid approach that combines linguistic cue and machine learning, with network-based behavioral data. Although designing a fake news detector is not a straightforward problem, we propose operational guidelines for a feasible fake news detecting system.},
  year     = {2015}
}

@article{TheFakeNewsSpreadingPlague_Mustafaraj,
  author     = {Eni Mustafaraj and
                Panagiotis Takis Metaxas},
  title      = {The Fake News Spreading Plague: Was it Preventable?},
  journal    = {CoRR},
  volume     = {abs/1703.06988},
  year       = {2017},
  url        = {http://arxiv.org/abs/1703.06988},
  eprinttype = {arXiv},
  eprint     = {1703.06988},
  timestamp  = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/MustafarajM17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{WhenFakeNewsBecomesReal_Balmas,
  author   = {Meital Balmas},
  title    = {When Fake News Becomes Real: Combined Exposure to Multiple News Sources and Political Attitudes of Inefficacy, Alienation, and Cynicism},
  journal  = {Communication Research},
  volume   = {41},
  number   = {3},
  pages    = {430-454},
  year     = {2014},
  doi      = {10.1177/0093650212453600},
  url      = { 
              https://doi.org/10.1177/0093650212453600
              
              },
  eprint   = { 
              https://doi.org/10.1177/0093650212453600
              
              },
  abstract = { This research assesses possible associations between viewing fake news (i.e., political satire) and attitudes of inefficacy, alienation, and cynicism toward political candidates. Using survey data collected during the 2006 Israeli election campaign, the study provides evidence for an indirect positive effect of fake news viewing in fostering the feelings of inefficacy, alienation, and cynicism, through the mediator variable of perceived realism of fake news. Within this process, hard news viewing serves as a moderator of the association between viewing fake news and their perceived realism. It was also demonstrated that perceived realism of fake news is stronger among individuals with high exposure to fake news and low exposure to hard news than among those with high exposure to both fake and hard news. Overall, this study contributes to the scientific knowledge regarding the influence of the interaction between various types of media use on political effects. }
}

@article{TheImpactOfRealNewsAboutFakeNews_Brewer,
  author   = {Brewer, Paul R. and Young, Dannagal Goldthwaite and Morreale, Michelle},
  title    = {{The Impact of Real News about “Fake News”: Intertextual Processes and Political Satire}},
  journal  = {International Journal of Public Opinion Research},
  volume   = {25},
  number   = {3},
  pages    = {323-343},
  year     = {2013},
  month    = {09},
  abstract = {{This study builds on research about political humor, press metacoverage, and intertextuality to examine the effects of news coverage about political satire on audience members. The analysis uses experimental data to test whether news coverage of Stephen Colbert’s Super PAC influenced knowledge and opinion regarding Citizens United, as well as political trust and internal political efficacy. It also tests whether such effects depended on previous exposure to The Colbert Report (Colbert’s satirical television show) and traditional news. Results indicate that exposure to news coverage of satire can influence knowledge, opinion, and political trust. Additionally, regular satire viewers may experience stronger effects on opinion, as well as increased internal efficacy, when consuming news coverage about issues previously highlighted in satire programming.}},
  issn     = {0954-2892},
  doi      = {10.1093/ijpor/edt015},
  url      = {https://doi.org/10.1093/ijpor/edt015},
  eprint   = {https://academic.oup.com/ijpor/article-pdf/25/3/323/2358632/edt015.pdf}
}

@article{NewsVerificationByExploitingConflictingSocialViewpoints_Jin,
  title        = {News Verification by Exploiting Conflicting Social Viewpoints in Microblogs},
  volume       = {30},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/10382},
  doi          = {10.1609/aaai.v30i1.10382},
  abstractnote = { &lt;p&gt; Fake news spreading in social media severely jeopardizes the veracity of online content. Fortunately, with the interactive and open features of microblogs, skeptical and opposing voices against fake news always arise along with it. The conflicting information, ignored by existing studies, is crucial for news verification. In this paper, we take advantage of this &quot;wisdom of crowds&quot; information to improve news verification by mining conflicting viewpoints in microblogs. First, we discover conflicting viewpoints in news tweets with a topic model method. Based on identified tweets’ viewpoints, we then build a credibility propagation network of tweets linked with supporting or opposing relations. Finally, with iterative deduction, the credibility propagation on the network generates the final evaluation result for news. Experiments conducted on a real-world data set show that the news verification performance of our approach significantly outperforms those of the baseline approaches. &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Jin, Zhiwei and Cao, Juan and Zhang, Yongdong and Luo, Jiebo},
  year         = {2016},
  month        = {Mar.}
}

@inproceedings{FakeNewsOrTruthUsingSatiricalCues_Rubin,
  title     = {Fake News or Truth? Using Satirical Cues to Detect Potentially Misleading News},
  author    = {Rubin, Victoria  and
               Conroy, Niall  and
               Chen, Yimin  and
               Cornwell, Sarah},
  booktitle = {Proceedings of the Second Workshop on Computational Approaches to Deception Detection},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W16-0802},
  doi       = {10.18653/v1/W16-0802},
  pages     = {7--17}
}

@article{DeceptionDetectionForFakeNews3TypesOfFakeNews_Rubin,
  author   = {Rubin, Victoria L. and Chen, Yimin and Conroy, Nadia K.},
  title    = {Deception detection for news: Three types of fakes},
  journal  = {Proceedings of the Association for Information Science and Technology},
  volume   = {52},
  number   = {1},
  pages    = {1-4},
  keywords = {news verification, deception detection, fake news detection, credibility assessment, reputable sources, fabrication, hoax, satire, natural language processing, text analytics, predictive modeling, corpus construction},
  doi      = {https://doi.org/10.1002/pra2.2015.145052010083},
  url      = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.2015.145052010083},
  eprint   = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.2015.145052010083},
  abstract = {ABSTRACT A fake news detection system aims to assist users in detecting and filtering out varieties of potentially deceptive news. The prediction of the chances that a particular news item is intentionally deceptive is based on the analysis of previously seen truthful and deceptive news. A scarcity of deceptive news, available as corpora for predictive modeling, is a major stumbling block in this field of natural language processing (NLP) and deception detection. This paper discusses three types of fake news, each in contrast to genuine serious reporting, and weighs their pros and cons as a corpus for text analytics and predictive modeling. Filtering, vetting, and verifying online information continues to be essential in library and information science (LIS), as the lines between traditional news and online information are blurring.},
  year     = {2015}
}

@article{ConspiracyTheories_Sunstein,
  author  = {Sunstein, Cass R. and Vermeule, Adrian},
  title   = {Conspiracy Theories: Causes and Cures*},
  journal = {Journal of Political Philosophy},
  volume  = {17},
  number  = {2},
  pages   = {202-227},
  doi     = {https://doi.org/10.1111/j.1467-9760.2008.00325.x},
  url     = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9760.2008.00325.x},
  eprint  = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9760.2008.00325.x},
  year    = {2009}
}

@article{Superstition_Lindeman,
  title    = {Superstitious, magical, and paranormal beliefs: An integrative model},
  journal  = {Journal of Research in Personality},
  volume   = {41},
  number   = {4},
  pages    = {731-744},
  year     = {2007},
  issn     = {0092-6566},
  doi      = {https://doi.org/10.1016/j.jrp.2006.06.009},
  url      = {https://www.sciencedirect.com/science/article/pii/S0092656606000869},
  author   = {Marjaana Lindeman and Kia Aarnio},
  keywords = {Superstition, Magical thinking, Paranormal beliefs, Intuitive thinking, Core knowledge},
  abstract = {Lack of conceptual clarity has hampered theory formation and research on superstitious, magical, and paranormal beliefs. This study offers a conceptual framework where these concepts are differentiated from other unfounded beliefs and defined identically as a confusion of core knowledge about physical, psychological, and biological phenomena. When testing this definition with questionnaire items (N=239), the results showed that superstitious individuals accepted more violations of core ontological distinctions than skeptics did and that ontological confusions discriminated believers from skeptics better than intuitive thinking, analytical thinking, or emotional instability. The findings justify the present conceptualization of superstitious, magical, and paranormal beliefs, and offer new theoretical propositions for the familiar everyday beliefs that are yet scientifically so poorly understood.}
}

@article{RumorsAndHealthCareReform_Berinsky,
  title    = {Rumors and Health Care Reform: Experiments in Political Misinformation},
  author   = {Berinsky, Adam J.},
  year     = {2017},
  journal  = {British Journal of Political Science},
  volume   = {47},
  number   = {2},
  pages    = {241-262},
  abstract = {This article explores belief in political rumors surrounding the health care reforms enacted by Congress in 2010. Refuting rumors with statements from unlikely sources can, under certain circumstances, increase the willingness of citizens to reject rumors regardless of their own political predilections. Such source credibility effects, while well known in the political persuasion literature, have not been applied to the study of rumor. Though source credibility appears to be an effective tool for debunking political rumors, risks remain. Drawing upon research from psychology on â€˜fluencyâ€™ â€“ the ease of information recall â€“ this article argues that rumors acquire power through familiarity. Attempting to quash rumors through direct refutation may facilitate their diffusion by increasing fluency. The empirical results find that merely repeating a rumor increases its power.},
  url      = {https://EconPapers.repec.org/RePEc:cup:bjposi:v:47:y:2017:i:02:p:241-262_00}
}

﻿@article{WhenCorrectionsFail_Nyhan,
  author   = {Nyhan, Brendan
              and Reifler, Jason},
  title    = {When Corrections Fail: The Persistence of Political Misperceptions},
  journal  = {Political Behavior},
  year     = {2010},
  month    = {Jun},
  day      = {01},
  volume   = {32},
  number   = {2},
  pages    = {303-330},
  abstract = {An extensive literature addresses citizen ignorance, but very little research focuses on misperceptions. Can these false or unsubstantiated beliefs about politics be corrected? Previous studies have not tested the efficacy of corrections in a realistic format. We conducted four experiments in which subjects read mock news articles that included either a misleading claim from a politician, or a misleading claim and a correction. Results indicate that corrections frequently fail to reduce misperceptions among the targeted ideological group. We also document several instances of a ``backfire effect'' in which corrections actually increase misperceptions among the group in question.},
  issn     = {1573-6687},
  doi      = {10.1007/s11109-010-9112-2},
  url      = {https://doi.org/10.1007/s11109-010-9112-2}
}

@article{ProspectTheory_Kahneman,
  title   = {Prospect theory: analysis of decision under risk},
  author  = {Daniel Kahneman and Amos Tversky},
  journal = {Econometrica},
  year    = {1979},
  volume  = {47},
  pages   = {263-291}
}

@article{AdvancesInProspectTheory_Kahneman,
  title   = {Advances in prospect theory: Cumulative representation of uncertainty},
  author  = {Amos Tversky and Daniel Kahneman},
  journal = {Journal of Risk and Uncertainty},
  year    = {1992},
  volume  = {5},
  pages   = {297-323}
}

@article{TheEffectOfPeopleRecommenderOnEchoChambers_Cinus,
  title        = {The Effect of People Recommenders on Echo Chambers and Polarization},
  volume       = {16},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/19275},
  abstractnote = {The effects of online social media on critical issues, such as polarization and misinformation, are under scrutiny due to the disruptive consequences that these phenomena can have on our societies. Among the algorithms routinely used by social media platforms, people-recommender systems are of special interest, as they directly contribute to the evolution of the social network structure, affecting the information and the opinions users are exposed to. In this paper, we propose a novel framework to assess the effect of people recommenders on the evolution of opinions. Our proposal is based on Monte Carlo simulations combining link recommendation and opinion-dynamics models. In order to control initial conditions, we define a random network model to generate graphs with opinions, with tunable amounts of modularity and homophily. Finally, we join these elements into a methodology able to study the causal relationship between the recommender system and the echo chamber effect. Our method can also assess if such relationships are statistically significant. We also show how such a framework can be used to measure, by means of simulations, the impact of different intervention strategies. Our thorough experimentation shows that people recommenders can in fact lead to a significant increase in echo chambers. However, this happens only if there is considerable initial homophily in the network. Also, we find that if the network already contains echo chambers, the effect of the recommendation algorithm is negligible. Such findings are robust to two very different opinion dynamics models, a bounded confidence model and an epistemological model.},
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Cinus, Federico and Minici, Marco and Monti, Corrado and Bonchi, Francesco},
  year         = {2022},
  month        = {May},
  pages        = {90-101}
}

@inproceedings{TheRussianFirehoseOfFalsehood_Paul,
  title  = {The Russian "Firehose of Falsehood" Propaganda Model: Why It Might Work and Options to Counter It},
  author = {Christopher Paul and Miriam Matthews},
  year   = {2016}
}

@article{TheRiseOfSocialBots_Ferrara,
  author     = {Ferrara, Emilio and Varol, Onur and Davis, Clayton and Menczer, Filippo and Flammini, Alessandro},
  title      = {The Rise of Social Bots},
  year       = {2016},
  issue_date = {July 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {59},
  number     = {7},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/2818717},
  doi        = {10.1145/2818717},
  abstract   = {Today's social bots are sophisticated and sometimes menacing. Indeed, their presence can endanger online ecosystems as well as our society.},
  journal    = {Commun. ACM},
  month      = {jun},
  pages      = {96–104},
  numpages   = {9}
}

 @article{SocialBotsDistortThe2016USPresidentialElection_Bessi,
  title   = {Social Bots Distort the 2016 US Presidential Election Online Discussion},
  volume  = {21},
  number  = {11},
  journal = {First Monday},
  author  = {Bessi, Alessandro and Ferrara, Emilio},
  year    = {2016},
  month   = {Nov}
} Available at SSRN: https://ssrn.com/abstract=2982233

@inproceedings{AnyoneCanBecomeATroll_Cheng,
  author    = {Cheng, Justin and Bernstein, Michael and Danescu-Niculescu-Mizil, Cristian and Leskovec, Jure},
  title     = {Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions},
  year      = {2017},
  isbn      = {9781450343350},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2998181.2998213},
  doi       = {10.1145/2998181.2998213},
  abstract  = {In online communities, antisocial behavior such as trolling disrupts constructive discussion. While prior work suggests that trolling behavior is confined to a vocal and antisocial minority, we demonstrate that ordinary people can engage in such behavior as well. We propose two primary trigger mechanisms: the individual's mood, and the surrounding context of a discussion (e.g., exposure to prior trolling behavior). Through an experiment simulating an online discussion, we find that both negative mood and seeing troll posts by others significantly increases the probability of a user trolling, and together double this probability. To support and extend these results, we study how these same mechanisms play out in the wild via a data-driven, longitudinal analysis of a large online news discussion community. This analysis exposes temporal mood effects, and explores long range patterns of repeated exposure to trolling. A predictive model of trolling behavior reveals that mood and discussion context together can explain trolling behavior better than an individual's history of trolling. These results combine to suggest that ordinary people can, under the right circumstances, behave like trolls.},
  booktitle = {Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
  pages     = {1217–1230},
  numpages  = {14},
  keywords  = {online communities, antisocial behavior, trolling},
  location  = {Portland, Oregon, USA},
  series    = {CSCW '17}
}

@article{NewsInAnOnlineWorld_Chen,
  author   = {Chen, Yimin and Conroy, Nadia K. and Rubin, Victoria L.},
  title    = {News in an online world: The need for an “automatic crap detector”},
  journal  = {Proceedings of the Association for Information Science and Technology},
  volume   = {52},
  number   = {1},
  pages    = {1-4},
  keywords = {News verification, deception detection, fake news detection, credibility assessment, journalism practices, online news, media commercialization, natural language processing},
  doi      = {https://doi.org/10.1002/pra2.2015.145052010081},
  url      = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.2015.145052010081},
  eprint   = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.2015.145052010081},
  abstract = {ABSTRACT Widespread adoption of internet technologies has changed the way that news is created and consumed. The current online news environment is one that incentivizes speed and spectacle in reporting, at the cost of fact-checking and verification. The line between user generated content and traditional news has also become increasingly blurred. This poster reviews some of the professional and cultural issues surrounding online news and argues for a two-pronged approach inspired by Hemingway's “automatic crap detector” (Manning, 1965) in order to address these problems: a) proactive public engagement by educators, librarians, and information specialists to promote digital literacy practices; b) the development of automated tools and technologies to assist journalists in vetting, verifying, and fact-checking, and to assist news readers by filtering and flagging dubious information.},
  year     = {2015}
}

@inproceedings{TowardsNewsVerification_Rubin,
  author = {Rubin, Victoria and Conroy, Nadia and Chen, Yimin},
  year   = {2015},
  month  = {01},
  pages  = {},
  title  = {Towards News Verification: Deception Detection Methods for News Discourse},
  doi    = {10.13140/2.1.4822.8166}
}

@book{DieEntwicklungDerGerichtspsychologischen_Undeutsch,
  author    = {Undeutsch, Udo},
  title     = {Die Entwicklung der gerichtspsychologischen Gutachtertaetigkeit},
  publisher = {Hogrefe},
  address   = {Goettingen},
  year      = {1954},
  pages     = {32 S.},
  language  = {ger},
  library   = {JU [Signatur: JU/PH 9020 U56]}
}

 @book{SCAN_Sapir1987,
  place     = {Phoenix, AZ},
  journal   = {Scientific Content Analysis (SCAN)},
  title     = {Scientific Content Analysis ({SCAN})},
  publisher = {Laboratory of Scientific Interrogation},
  author    = {Sapir, A},
  year      = {1987}
}

 @book{SCAN_Smith2001,
  place     = {London},
  title     = {Reading between the lines: An evauluation of the Scientific Content Analysis Techniques (SCAN)},
  publisher = {Home Office},
  author    = {Smith, Nicky},
  year      = {2001}
}

@article{TheAccuracyConfidenceRelation_DePaulo,
  author   = {Bella M. DePaulo and Kelly Charlton and Harris Cooper and James J. Lindsay and Laura Muhlenbruck},
  title    = {The Accuracy-Confidence Correlation in the Detection of Deception},
  journal  = {Personality and Social Psychology Review},
  volume   = {1},
  number   = {4},
  pages    = {346-357},
  year     = {1997},
  doi      = {10.1207/s15327957pspr0104\_5},
  note     = {PMID: 15661668},
  url      = {https://doi.org/10.1207/s15327957pspr0104_5},
  eprint   = {https://doi.org/10.1207/s15327957pspr0104_5},
  abstract = {A meta-analysis was conducted of research on the relation between judges' accuracy at detecting deception and their confidence in their judgments. A total of 18 independent samples revealed an average weighted accuracy-confidence correlation of .04, a relation not significantly different from zero. However, confidence was positively correlated with judges' tendency to perceive messages as truthful, regardless of the actual truthfulness of the messages. Judges were also more confident when they really were rating truths compared to when they were rating lies. Also, men were more confident than women, and judges who had a closer relationship to the message sender felt more confident in their judgments of truths and lies. Methodological and theoretical explanations for these findings are discussed. }
}

 @misc{CommunicationUnderStress_Adams,
  title     = {Communication under stress: Indicators of veracity and deception in written narratives},
  url       = {http://hdl.handle.net/10919/11057},
  journal   = {VTechWorks Home},
  publisher = {Virginia Tech},
  author    = {Adams, Susan H.},
  year      = {2002},
  month     = {Apr}
} 

@article{LyingWords_Newman,
  author   = {Matthew L. Newman and James W. Pennebaker and Diane S. Berry and Jane M. Richards},
  title    = {Lying Words: Predicting Deception from Linguistic Styles},
  journal  = {Personality and Social Psychology Bulletin},
  volume   = {29},
  number   = {5},
  pages    = {665-675},
  year     = {2003},
  doi      = {10.1177/0146167203029005010},
  note     = {PMID: 15272998},
  url      = {https://doi.org/10.1177/0146167203029005010},
  eprint   = {https://doi.org/10.1177/0146167203029005010},
  abstract = {Telling lies often requires creating a story about an experience or attitude that does not exist. As a result, false stories may be qualitatively different from true stories. The current project investigated the features of linguistic style that distinguish between true and false stories. In an analysis of five independent samples, a computer-based text analysis program correctly classified liars and truth-tellers at a rate of 67\% when the topic was constant and a rate of 61\% overall. Compared to truth-tellers, liars showed lower cognitive complexity, used fewer self-references and other-references, and used more negative emotion words. }
}

@inproceedings{VerificatoinAndImplementationofLBDeceptionIndicators_Bachenko,
  title     = {Verification and Implementation of Language-Based Deception Indicators in Civil and Criminal Narratives},
  author    = {Bachenko, Joan  and
               Fitzpatrick, Eileen  and
               Schonwetter, Michael},
  booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  month     = {08},
  year      = {2008},
  address   = {Manchester, UK},
  publisher = {Coling 2008 Organizing Committee},
  url       = {https://aclanthology.org/C08-1006},
  pages     = {41--48}
}

@book{ClassificationRegressioniTrees_Breiman,
  title     = {Classification and Regression Trees},
  author    = {Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn      = {9780412048418},
  lccn      = {83019708},
  url       = {https://books.google.de/books?id=JwQx-WOmSyQC},
  year      = {1984},
  publisher = {Taylor \& Francis}
}

@article{DecisionSupportForDeterminingVeracity_Fuller,
  title    = {Decision support for determining veracity via linguistic-based cues},
  journal  = {Decision Support Systems},
  volume   = {46},
  number   = {3},
  pages    = {695-703},
  year     = {2009},
  note     = {Wireless in the Healthcare},
  issn     = {0167-9236},
  doi      = {https://doi.org/10.1016/j.dss.2008.11.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167923608001991},
  author   = {Christie M. Fuller and David P. Biros and Rick L. Wilson},
  keywords = {Deception, Deception detection, Credibility assessment, Classification, Linguistic-based cues, Decision support systems, Neural networks, Decision trees, Logistic regression},
  abstract = {Deception detection is an essential skill in careers such as law enforcement and must be accomplished accurately. However, humans are not very competent at determining veracity without aid. This study examined automated text-based deception detection which attempts to overcome the shortcomings of previous credibility assessment methods. A real-world, high-stakes sample of statements was collected and analyzed. Several different sets of linguistic-based cues were used as inputs for classification models. Overall accuracy rates of up to 74% were achieved, suggesting that automated deception detection systems can be an invaluable tool for those who must assess the credibility of text.}
}

@article{OnLyingAndBeingLiedTo_Hancock,
  author    = { Jeffrey T.   Hancock  and  Lauren E.   Curry  and  Saurabh   Goorha  and  Michael   Woodworth },
  title     = {On Lying and Being Lied To: A Linguistic Analysis of Deception in Computer-Mediated Communication},
  journal   = {Discourse Processes},
  volume    = {45},
  number    = {1},
  pages     = {1-23},
  year      = {2007},
  publisher = {Routledge},
  doi       = {10.1080/01638530701739181},
  url       = {https://doi.org/10.1080/01638530701739181},
  eprint    = {https://doi.org/10.1080/01638530701739181}
}

@inproceedings{OnDeceptionAndDeceptionDetection_Rubin,
  author    = {Rubin, Victoria L.},
  title     = {On Deception and Deception Detection: Content Analysis of Computer-Mediated Stated Beliefs},
  year      = {2010},
  publisher = {American Society for Information Science},
  address   = {USA},
  abstract  = {Deception in computer-mediated communication is defined as a message knowingly and intentionally transmitted by a sender to foster a false belief or conclusion by the perceiver. Stated beliefs about deception and deceptive messages or incidents are content analyzed in a sample of 324 computer-mediated communications. Relevant stated beliefs are obtained through systematic sampling and querying of the blogosphere based on 80 English words commonly used to describe deceptive incidents. Deception is conceptualized broader than lying and includes a variety of deceptive strategies: falsification, concealment (omitting material facts) and equivocation (dodging or skirting issues). The stated beliefs are argued to be valuable toward the creation of a unified multi-faceted ontology of deception, stratified along several classificatory facets such as (1) contextual domain (e.g., personal relations, politics, finances and insurance), (2) deception content (e.g., events, time, place, abstract notions), (3) message format (e.g., a complaint: they lied to us, a victim story: I was lied to or tricked, or a direct accusation: you're lying), and (4) deception variety, each tied to particular verbal cues (e.g., misinforming, scheming, misrepresenting, or cheating). The paper positions automated deception detection within the field of library and information science (LIS), as a feasible natural language processing (NLP) task. Key findings and important constructs in deception research from interpersonal communication, psychology, criminology, and language technology studies are synthesized into an overview. Deception research is juxtaposed to several benevolent constructs in LIS research: trust, credibility, certainty, and authority.},
  booktitle = {Proceedings of the 73rd ASIST Annual Meeting on Navigating Streams in an Information Ecosystem - Volume 47},
  articleno = {32},
  numpages  = {10},
  keywords  = {trust, computer-mediated communications, credibility, natural language processing, information security, content analysis, blogs, deception detection, automated text classification},
  location  = {Pittsburgh, Pennsylvania},
  series    = {ASIST '10}
}

@article{AutomatingLinguisticsBasedCues_Zhou,
  author   = {Zhou, Lina
              and Burgoon, Judee K.
              and Nunamaker, Jay F.
              and Twitchell, Doug},
  title    = {Automating Linguistics-Based Cues for Detecting Deception in Text-Based Asynchronous Computer-Mediated Communications},
  journal  = {Group Decision and Negotiation},
  year     = {2004},
  month    = {Jan},
  day      = {01},
  volume   = {13},
  number   = {1},
  pages    = {81-106},
  abstract = {The detection of deception is a promising but challenging task. A systematic discussion of automated Linguistics Based Cues (LBC) to deception has rarely been touched before. The experiment studied the effectiveness of automated LBC in the context of text-based asynchronous computer mediated communication (TA-CMC). Twenty-seven cues either extracted from the prior research or created for this study were clustered into nine linguistics constructs: quantity, diversity, complexity, specificity, expressivity, informality, affect, uncertainty, and nonimmediacy. A test of the selected LBC in a simulated TA-CMC experiment showed that: (1) a systematic analysis of linguistic information could be useful in the detection of deception; (2) some existing LBC were effective as expected, while some others turned out in the opposite direction to the prediction of the prior research; and (3) some newly discovered linguistic constructs and their component LBC were helpful in differentiating deception from truth.},
  issn     = {1572-9907},
  doi      = {10.1023/B:GRUP.0000011944.62889.6f},
  url      = {https://doi.org/10.1023/B:GRUP.0000011944.62889.6f}
}

@inproceedings{iSkim_Zhou,
  author    = {Lina Zhou and Booker, Q.E. and Dongsong Zhang},
  booktitle = {Proceedings of the 35th Annual Hawaii International Conference on System Sciences},
  title     = {ROD - toward rapid ontology development for underdeveloped domains},
  year      = {2002},
  volume    = {},
  number    = {},
  pages     = {957-965},
  doi       = {10.1109/HICSS.2002.994046}
}

@inproceedings{IdentificationOfTruth_Rubin,
  author    = {Rubin, Victoria L. and Vashchilko, Tatiana},
  title     = {Identification of Truth and Deception in Text: Application of Vector Space Model to Rhetorical Structure Theory},
  year      = {2012},
  publisher = {Association for Computational Linguistics},
  address   = {USA},
  abstract  = {The paper proposes to use Rhetorical Structure Theory (RST) analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self-ranked as completely truthful or completely deceptive, is manually analyzed by assigning RST discourse relations among a story's constituent parts. Vector Space Model (VSM) assesses each story's position in multi-dimensional RST space with respect to its distance to truth and deceptive centers as measures of the story's level of deception and truthfulness. Ten human judges evaluate if each story is deceptive or not, and assign their confidence levels, which produce measures of the human expected deception and truthfulness levels. The paper contributes to deception detection research and RST twofold: a) demonstration of discourse structure analysis in pragmatics as a prominent way of automated deception detection and, as such, an effective complement to lexico-semantic analysis, and b) development of RST-VSM methodology to interpret RST analysis in identification of previously unseen deceptive texts.},
  booktitle = {Proceedings of the Workshop on Computational Approaches to Deception Detection},
  pages     = {97–106},
  numpages  = {10},
  location  = {Avignon, France},
  series    = {EACL 2012}
}

@article{TruthAndDeception_Rubin,
  author     = {Rubin, Victoria L. and Lukoianova, Tatiana},
  title      = {Truth and Deception at the Rhetorical Structure Level},
  year       = {2015},
  issue_date = {May 2015},
  publisher  = {John Wiley &amp; Sons, Inc.},
  address    = {USA},
  volume     = {66},
  number     = {5},
  issn       = {2330-1635},
  url        = {https://doi.org/10.1002/asi.23216},
  doi        = {10.1002/asi.23216},
  abstract   = {This paper furthers the development of methods to distinguish truth from deception in textual data. We use rhetorical structure theory RST as the analytic framework to identify systematic differences between deceptive and truthful stories in terms of their coherence and structure. A sample of 36 elicited personal stories, self-ranked as truthful or deceptive, is manually analyzed by assigning RST discourse relations among each story's constituent parts. A vector space model VSM assesses each story's position in multidimensional RST space with respect to its distance from truthful and deceptive centers as measures of the story's level of deception and truthfulness. Ten human judges evaluate independently whether each story is deceptive and assign their confidence levels 360 evaluations total, producing measures of the expected human ability to recognize deception. As a robustness check, a test sample of 18 truthful stories with 180 additional evaluations is used to determine the reliability of our RST-VSM method in determining deception. The contribution is in demonstration of the discourse structure analysis as a significant method for automated deception detection and an effective complement to lexicosemantic analysis. The potential is in developing novel discourse-based tools to alert information users to potential deception in computer-mediated texts.},
  journal    = {J. Assoc. Inf. Sci. Technol.},
  month      = {may},
  pages      = {905–917},
  numpages   = {13},
  keywords   = {computer mediated communications, discourse analysis, natural language processing}
}

@article{RST_William,
  url         = {https://doi.org/10.1515/text.1.1988.8.3.243},
  title       = {Rhetorical Structure Theory: Toward a functional theory of text organization},
  title       = {},
  author      = {William C. Mann and Sandra A. Thompson},
  pages       = {243--281},
  volume      = {8},
  number      = {3},
  journal     = {Text - Interdisciplinary Journal for the Study of Discourse},
  doi         = {doi:10.1515/text.1.1988.8.3.243},
  year        = {1988},
  lastchecked = {2022-09-24}
}

﻿@article{C45_Salzberg,
  author  = {Salzberg, Steven L.},
  title   = {C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},
  journal = {Machine Learning},
  year    = {1994},
  month   = {Sep},
  day     = {01},
  volume  = {16},
  number  = {3},
  pages   = {235-240},
  issn    = {1573-0565},
  doi     = {10.1007/BF00993309},
  url     = {https://doi.org/10.1007/BF00993309}
}

@inproceedings{DetecingDeceptionThroughLA_Burgoon,
  author    = {Burgoon, Judee K.
               and Blair, J. P.
               and Qin, Tiantian
               and Nunamaker, Jay F.},
  editor    = {Chen, Hsinchun
               and Miranda, Richard
               and Zeng, Daniel D.
               and Demchak, Chris
               and Schroeder, Jenny
               and Madhusudan, Therani},
  title     = {Detecting Deception through Linguistic Analysis},
  booktitle = {Intelligence and Security Informatics},
  year      = {2003},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {91--101},
  abstract  = {Tools to detect deceit from language use pose a promising avenue for increasing the ability to distinguish truthful transmissions, transcripts, intercepted messages, informant reports and the like from deceptive ones. This investigation presents preliminary tests of 16 linguistic features that can be automated to return assessments of the likely truthful or deceptiveness of a piece of text. Results from a mock theft experiment demonstrate that deceivers do utilize language differently than truth tellers and that combinations of cues can improve the ability to predict which texts may contain deception.},
  isbn      = {978-3-540-44853-2}
}

@inproceedings{DetectingHoaxesFraudsAndDeception_Afroz,
  author    = {Afroz, Sadia and Brennan, Michael and Greenstadt, Rachel},
  booktitle = {2012 IEEE Symposium on Security and Privacy},
  title     = {Detecting Hoaxes, Frauds, and Deception in Writing Style Online},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {461-475},
  doi       = {10.1109/SP.2012.34}
}

@inproceedings{LIWC2007_Pennebaker,
  title  = {Linguistic Inquiry and Word Count (LIWC2007)},
  author = {James W. Pennebaker and Roger John Booth and Martha E. Francis},
  year   = {2007}
}

@inbook{POS_Daelemans,
  author    = {Daelemans, Walter},
  editor    = {Sammut, Claude
               and Webb, Geoffrey I.},
  title     = {POS Tagging},
  booktitle = {Encyclopedia of Machine Learning},
  year      = {2010},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {776--779},
  isbn      = {978-0-387-30164-8},
  doi       = {10.1007/978-0-387-30164-8_643},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_643}
}

@article{SVM_Hearst,
  author  = {Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal = {IEEE Intelligent Systems and their Applications},
  title   = {Support vector machines},
  year    = {1998},
  volume  = {13},
  number  = {4},
  pages   = {18-28},
  doi     = {10.1109/5254.708428}
}

@article{TF_Luhn,
  author  = {Luhn, H. P.},
  journal = {IBM Journal of Research and Development},
  title   = {A Statistical Approach to Mechanized Encoding and Searching of Literary Information},
  year    = {1957},
  volume  = {1},
  number  = {4},
  pages   = {309-317},
  doi     = {10.1147/rd.14.0309}
}

@article{IDF_Jones,
  author  = {Karen Spärck Jones},
  title   = {A statistical interpretation of term specificity and its application in retrieval},
  journal = {Journal of Documentation},
  year    = {1972},
  volume  = {28},
  pages   = {11--21}
}

@article{AStylometricInquiry_Potthast,
  author     = {Martin Potthast and
                Johannes Kiesel and
                Kevin Reinartz and
                Janek Bevendorff and
                Benno Stein},
  title      = {A Stylometric Inquiry into Hyperpartisan and Fake News},
  journal    = {CoRR},
  volume     = {abs/1702.05638},
  year       = {2017},
  url        = {http://arxiv.org/abs/1702.05638},
  eprinttype = {arXiv},
  eprint     = {1702.05638},
  timestamp  = {Wed, 30 Oct 2019 15:47:55 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/PotthastKRBS17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{VerifyingMultimediaUse_Boididou,
  author = {Boididou, Christina and Papadopoulos, Symeon and Dang Nguyen, Duc Tien and Boato, G. and Riegler, Michael and Petlund, Andreas and Kompatsiaris, Ioannis},
  year   = {2016},
  month  = {10},
  pages  = {},
  title  = {Verifying Multimedia Use at MediaEval 2016}
}

@article{VisualMisAndDisinformation_Viorela,
  author  = {Viorela Dan and Britt Paris and Joan Donovan and Michael Hameleers and Jon Roozenbeek and Sander van der Linden and Christian von Sikorski},
  title   = {Visual Mis- and Disinformation, Social Media, and Democracy},
  journal = {Journalism \& Mass Communication Quarterly},
  volume  = {98},
  number  = {3},
  pages   = {641-664},
  year    = {2021},
  doi     = {10.1177/10776990211035395},
  url     = {https://doi.org/10.1177/10776990211035395},
  eprint  = {https://doi.org/10.1177/10776990211035395
             }
}

 @misc{PutinBehindBars_Harding,
  title     = {Putin seen behind bars in spoof video},
  url       = {https://www.theguardian.com/world/2012/feb/15/putin-behind-bars-spoof-video},
  journal   = {The Guardian},
  publisher = {Guardian News and Media},
  author    = {Harding, Luke},
  year      = {2012},
  month     = {Feb}
}

 @misc{DeepFakeQueensSpeech_Sawer,
  title     = {'deepfake' Queen's speech: Channel 4 criticised for 'disrespectful' Christmas message},
  url       = {https://www.telegraph.co.uk/news/2020/12/23/deepfake-queens-speech-channel-4-criticised-disrespectful-christmas/},
  journal   = {The Telegraph},
  publisher = {Telegraph Media Group},
  author    = {Sawer, Patrick},
  year      = {2020},
  month     = {Dec}
} 

@article{ExploitingMultiDomainVisualInformation_Qi,
  author     = {Peng Qi and
                Juan Cao and
                Tianyun Yang and
                Junbo Guo and
                Jintao Li},
  title      = {Exploiting Multi-domain Visual Information for Fake News Detection},
  journal    = {CoRR},
  volume     = {abs/1908.04472},
  year       = {2019},
  url        = {http://arxiv.org/abs/1908.04472},
  eprinttype = {arXiv},
  eprint     = {1908.04472},
  timestamp  = {Fri, 30 Jul 2021 15:26:18 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1908-04472.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{BeyondNewsContents_Shu,
  author    = {Shu, Kai and Wang, Suhang and Liu, Huan},
  title     = {Beyond News Contents: The Role of Social Context for Fake News Detection},
  year      = {2019},
  isbn      = {9781450359405},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3289600.3290994},
  doi       = {10.1145/3289600.3290994},
  abstract  = {Social media is becoming popular for news consumption due to its fast dissemination, easy access, and low cost. However, it also enables the wide propagation of fake news, i.e., news with intentionally false information. Detecting fake news is an important task, which not only ensures users receive authentic information but also helps maintain a trustworthy news ecosystem. The majority of existing detection algorithms focus on finding clues from news contents, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. Therefore, we need to explore auxiliary information to improve detection. The social context during news dissemination process on social media forms the inherent tri-relationship, the relationship among publishers, news pieces, and users, which has the potential to improve fake news detection. For example, partisan-biased publishers are more likely to publish fake news, and low-credible users are more likely to share fake news. In this paper, we study the novel problem of exploiting social context for fake news detection. We propose a tri-relationship embedding framework TriFN, which models publisher-news relations and user-news interactions simultaneously for fake news classification. We conduct experiments on two real-world datasets, which demonstrate that the proposed approach significantly outperforms other baseline methods for fake news detection.},
  booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
  pages     = {312–320},
  numpages  = {9},
  keywords  = {joint learning, fake news detection, social media mining},
  location  = {Melbourne VIC, Australia},
  series    = {WSDM '19}
}

@inproceedings{InformationCredibilityOnTwiter_Castillo,
  author    = {Castillo, Carlos and Mendoza, Marcelo and Poblete, Barbara},
  title     = {Information Credibility on Twitter},
  year      = {2011},
  isbn      = {9781450306324},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1963405.1963500},
  doi       = {10.1145/1963405.1963500},
  abstract  = {We analyze the information credibility of news propagated through Twitter, a popular microblogging service. Previous research has shown that most of the messages posted on Twitter are truthful, but the service is also used to spread misinformation and false rumors, often unintentionally.On this paper we focus on automatic methods for assessing the credibility of a given set of tweets. Specifically, we analyze microblog postings related to "trending" topics, and classify them as credible or not credible, based on features extracted from them. We use features from users' posting and re-posting ("re-tweeting") behavior, from the text of the posts, and from citations to external sources.We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings. Our results shows that there are measurable differences in the way messages propagate, that can be used to classify them automatically as credible or not credible, with precision and recall in the range of 70\% to 80\%.},
  booktitle = {Proceedings of the 20th International Conference on World Wide Web},
  pages     = {675–684},
  numpages  = {10},
  keywords  = {social media credibility, twitter, social media analytics},
  location  = {Hyderabad, India},
  series    = {WWW '11}
}

@inproceedings{AutomaticDetectionOfRumor_Yang,
  author    = {Yang, Fan and Liu, Yang and Yu, Xiaohui and Yang, Min},
  title     = {Automatic Detection of Rumor on Sina Weibo},
  year      = {2012},
  isbn      = {9781450315463},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2350190.2350203},
  doi       = {10.1145/2350190.2350203},
  abstract  = {The problem of gauging information credibility on social networks has received considerable attention in recent years. Most previous work has chosen Twitter, the world's largest micro-blogging platform, as the premise of research. In this work, we shift the premise and study the problem of information credibility on Sina Weibo, China's leading micro-blogging service provider. With eight times more users than Twitter, Sina Weibo is more of a Facebook-Twitter hybrid than a pure Twitter clone, and exhibits several important characteristics that distinguish it from Twitter. We collect an extensive set of microblogs which have been confirmed to be false rumors based on information from the official rumor-busting service provided by Sina Weibo. Unlike previous studies on Twitter where the labeling of rumors is done manually by the participants of the experiments, the official nature of this service ensures the high quality of the dataset. We then examine an extensive set of features that can be extracted from the microblogs, and train a classifier to automatically detect the rumors from a mixed set of true information and false information. The experiments show that some of the new features we propose are indeed effective in the classification, and even the features considered in previous studies have different implications with Sina Weibo than with Twitter. To the best of our knowledge, this is the first study on rumor analysis and detection on Sina Weibo.},
  booktitle = {Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics},
  articleno = {13},
  numpages  = {7},
  keywords  = {rumor detection, classification, Sina Weibo},
  location  = {Beijing, China},
  series    = {MDS '12}
}

@inproceedings{DetectRumorsUsingTimeSeries_Ma,
  author    = {Ma, Jing and Gao, Wei and Wei, Zhongyu and Lu, Yueming and Wong, Kam-Fai},
  title     = {Detect Rumors Using Time Series of Social Context Information on Microblogging Websites},
  year      = {2015},
  isbn      = {9781450337946},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2806416.2806607},
  doi       = {10.1145/2806416.2806607},
  abstract  = {Automatically identifying rumors from online social media especially microblogging websites is an important research issue. Most of existing work for rumor detection focuses on modeling features related to microblog contents, users and propagation patterns, but ignore the importance of the variation of these social context features during the message propagation over time. In this study, we propose a novel approach to capture the temporal characteristics of these features based on the time series of rumor's lifecycle, for which time series modeling technique is applied to incorporate various social context information. Our experiments using the events in two microblog datasets confirm that the method outperforms state-of-the-art rumor detection approaches by large margins. Moreover, our model demonstrates strong performance on detecting rumors at early stage after their initial broadcast.},
  booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
  pages     = {1751–-1754},
  numpages  = {4},
  keywords  = {temporal, rumor detection, time series, social context},
  location  = {Melbourne, Australia},
  series    = {CIKM '15}
}

@article{LatentDirichletAllocation_Blei,
  author     = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  title      = {Latent Dirichlet Allocation},
  year       = {2003},
  issue_date = {3/1/2003},
  publisher  = {JMLR.org},
  volume     = {3},
  number     = {null},
  issn       = {1532-4435},
  abstract   = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  journal    = {J. Mach. Learn. Res.},
  month      = {mar},
  pages      = {993–1022},
  numpages   = {30}
}

@inproceedings{DetectingRumorsFromMicroblogs_Ma,
  author    = {Ma, Jing and Gao, Wei and Mitra, Prasenjit and Kwon, Sejeong and Jansen, Bernard J. and Wong, Kam-Fai and Cha, Meeyoung},
  title     = {Detecting Rumors from Microblogs with Recurrent Neural Networks},
  year      = {2016},
  isbn      = {9781577357704},
  publisher = {AAAI Press},
  abstract  = {Microblogging platforms are an ideal place for spreading rumors and automatically debunking rumors is a crucial problem. To detect rumors, existing approaches have relied on hand-crafted features for employing machine learning algorithms that require daunting manual effort. Upon facing a dubious claim, people dispute its truthfulness by posting various cues over time, which generates long-distance dependencies of evidence. This paper presents a novel method that learns continuous representations of microblog events for identifying rumors. The proposed model is based on recurrent neural networks (RNN) for learning the hidden representations that capture the variation of contextual information of relevant posts over time. Experimental results on datasets from two real-world microblog platforms demonstrate that (1) the RNN method outperforms state-of-the-art rumor detection models that use hand-crafted features; (2) performance of the RNN-based algorithm is further improved via sophisticated recurrent units and extra hidden layers; (3) RNN-based method detects rumors more quickly and accurately than existing techniques, including the leading online rumor debunking services.},
  booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
  pages     = {3818–3824},
  numpages  = {7},
  location  = {New York, New York, USA},
  series    = {IJCAI'16}
}

@inproceedings{StanceClassificationAttention_Du,
  author    = {Jiachen Du and Ruifeng Xu and Yulan He and Lin Gui},
  title     = {Stance Classification with Target-specific Neural Attention},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {3988--3994},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/557},
  url       = {https://doi.org/10.24963/ijcai.2017/557}
}

@inproceedings{ProminentFeaturesOfRumorPropagation_Kwon,
  author    = {Kwon, Sejeong and Cha, Meeyoung and Jung, Kyomin and Chen, Wei and Wang, Yajun},
  booktitle = {2013 IEEE 13th International Conference on Data Mining},
  title     = {Prominent Features of Rumor Propagation in Online Social Media},
  year      = {2013},
  volume    = {},
  number    = {},
  pages     = {1103-1108},
  doi       = {10.1109/ICDM.2013.61}
}


@article{FightingMisinformationOnSocialMedia_Pennycook,
  author   = {Gordon Pennycook  and David G. Rand },
  title    = {Fighting misinformation on social media using crowdsourced judgments of news source quality},
  journal  = {Proceedings of the National Academy of Sciences},
  volume   = {116},
  number   = {7},
  pages    = {2521-2526},
  year     = {2019},
  doi      = {10.1073/pnas.1806781116},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1806781116},
  eprint   = {https://www.pnas.org/doi/pdf/10.1073/pnas.1806781116},
  abstract = {Reducing the spread of misinformation, especially on social media, is a major challenge. We investigate one potential approach: having social media platform algorithms preferentially display content from news sources that users rate as trustworthy. To do so, we ask whether crowdsourced trust ratings can effectively differentiate more versus less reliable sources. We ran two preregistered experiments (n = 1,010 from Mechanical Turk and n = 970 from Lucid) where individuals rated familiarity with, and trust in, 60 news sources from three categories: (i) mainstream media outlets, (ii) hyperpartisan websites, and (iii) websites that produce blatantly false content (“fake news”). Despite substantial partisan differences, we find that laypeople across the political spectrum rated mainstream sources as far more trustworthy than either hyperpartisan or fake news sources. Although this difference was larger for Democrats than Republicans—mostly due to distrust of mainstream sources by Republicans—every mainstream source (with one exception) was rated as more trustworthy than every hyperpartisan or fake news source across both studies when equally weighting ratings of Democrats and Republicans. Furthermore, politically balanced layperson ratings were strongly correlated (r = 0.90) with ratings provided by professional fact-checkers. We also found that, particularly among liberals, individuals higher in cognitive reflection were better able to discern between low- and high-quality sources. Finally, we found that excluding ratings from participants who were not familiar with a given news source dramatically reduced the effectiveness of the crowd. Our findings indicate that having algorithms up-rank content from trusted media outlets may be a promising approach for fighting the spread of misinformation on social media.}
}

@inproceedings{ClickbaitDetectionUsingDL_Agrawal,
  author    = {Agrawal, Amol},
  booktitle = {2016 2nd International Conference on Next Generation Computing Technologies (NGCT)},
  title     = {Clickbait detection using deep learning},
  year      = {2016},
  volume    = {},
  number    = {},
  pages     = {268-272},
  doi       = {10.1109/NGCT.2016.7877426}
}

@inproceedings{DivingDeepIntoClickbaits_Rony,
  author    = {Rony, Md Main Uddin and Hassan, Naeemul and Yousuf, Mohammad},
  title     = {Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?},
  year      = {2017},
  isbn      = {9781450349932},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3110025.3110054},
  doi       = {10.1145/3110025.3110054},
  abstract  = {The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3\%. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.},
  booktitle = {Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017},
  pages     = {232–239},
  numpages  = {8},
  location  = {Sydney, Australia},
  series    = {ASONAM '17}
}

@article{EnrichingWordVectorsWithSubwordInfo_Bojanowski,
  author     = {Piotr Bojanowski and
                Edouard Grave and
                Armand Joulin and
                Tomas Mikolov},
  title      = {Enriching Word Vectors with Subword Information},
  journal    = {CoRR},
  volume     = {abs/1607.04606},
  year       = {2016},
  url        = {http://arxiv.org/abs/1607.04606},
  eprinttype = {arXiv},
  eprint     = {1607.04606},
  timestamp  = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{BagOfTricksForTextClassificatoin_Joulin,
  author     = {Armand Joulin and
                Edouard Grave and
                Piotr Bojanowski and
                Tomas Mikolov},
  title      = {Bag of Tricks for Efficient Text Classification},
  journal    = {CoRR},
  volume     = {abs/1607.01759},
  year       = {2016},
  url        = {http://arxiv.org/abs/1607.01759},
  eprinttype = {arXiv},
  eprint     = {1607.01759},
  timestamp  = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/JoulinGBM16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DistributedRepresentationsOfWords_Mikolov,
  author     = {Tomas Mikolov and
                Ilya Sutskever and
                Kai Chen and
                Greg Corrado and
                Jeffrey Dean},
  title      = {Distributed Representations of Words and Phrases and their Compositionality},
  journal    = {CoRR},
  volume     = {abs/1310.4546},
  year       = {2013},
  url        = {http://arxiv.org/abs/1310.4546},
  eprinttype = {arXiv},
  eprint     = {1310.4546},
  timestamp  = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/MikolovSCCD13.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ClickbaitAndTabloidStrategies_Dolors,
  author  = {Palau-Sampio, Dolors},
  year    = {2016},
  month   = {4},
  pages   = {63-79},
  title   = {Reference press metamorphosis in the digital context: Clickbait and tabloid strategies in Elpais.com},
  volume  = {29},
  journal = {Communication \& Society},
  doi     = {10.15581/003.29.2.63-79}
}

@inproceedings{SemEvalHyperpartisanNewsDetection_Kiesel,
  title     = {{S}em{E}val-2019 Task 4: Hyperpartisan News Detection},
  author    = {Kiesel, Johannes  and
               Mestre, Maria  and
               Shukla, Rishabh  and
               Vincent, Emmanuel  and
               Adineh, Payam  and
               Corney, David  and
               Stein, Benno  and
               Potthast, Martin},
  booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
  month     = {06},
  year      = {2019},
  address   = {Minneapolis, Minnesota, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/S19-2145},
  doi       = {10.18653/v1/S19-2145},
  pages     = {829--839},
  abstract  = {Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one is able to reliably compute this meta information, news articles may be automatically tagged, this way encouraging or discouraging readers to consume the text. It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art. We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision. The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run. The best team achieved an accuracy of 0.822 on a balanced sample (yes : no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.}
}

@article{BERT_Devlin,
  author     = {Jacob Devlin and
                Ming{-}Wei Chang and
                Kenton Lee and
                Kristina Toutanova},
  title      = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                Understanding},
  journal    = {CoRR},
  volume     = {abs/1810.04805},
  year       = {2018},
  url        = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint     = {1810.04805},
  timestamp  = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@book{TheGeneralInquirer_Stone,
  author  = {Stone, Philip and Dunphy, Dexter and Smith, Marshall and Ogilvie, Daniel},
  year    = {1966},
  month   = {01},
  pages   = {},
  title   = {The General Inquirer: A Computer Approach to Content Analysis},
  volume  = {4},
  journal = {American Educational Research Journal - AMER EDUC RES J},
  doi     = {10.2307/1161774}
}

@inproceedings{FactChecking_Vlachos,
  title     = {Fact Checking: Task definition and dataset construction},
  author    = {Vlachos, Andreas  and
               Riedel, Sebastian},
  booktitle = {Proceedings of the {ACL} 2014 Workshop on Language Technologies and Computational Social Science},
  month     = jun,
  year      = {2014},
  address   = {Baltimore, MD, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W14-2508},
  doi       = {10.3115/v1/W14-2508},
  pages     = {18--22}
}

@article{AutomatedFactChecking_Thorne,
  author     = {James Thorne and
                Andreas Vlachos},
  title      = {Automated Fact Checking: Task formulations, methods and future directions},
  journal    = {CoRR},
  volume     = {abs/1806.07687},
  year       = {2018},
  url        = {http://arxiv.org/abs/1806.07687},
  eprinttype = {arXiv},
  eprint     = {1806.07687},
  timestamp  = {Tue, 03 Nov 2020 12:45:06 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1806-07687.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{OverviewOfCheckThat_Barroncede,
  author    = {Barr{\'o}n-Cede{\~{n}}o, Alberto
               and Elsayed, Tamer
               and Nakov, Preslav
               and Da San Martino, Giovanni
               and Hasanain, Maram
               and Suwaileh, Reem
               and Haouari, Fatima
               and Babulkov, Nikolay
               and Hamdan, Bayan
               and Nikolov, Alex
               and Shaar, Shaden
               and Ali, Zien Sheikh},
  editor    = {Arampatzis, Avi
               and Kanoulas, Evangelos
               and Tsikrika, Theodora
               and Vrochidis, Stefanos
               and Joho, Hideo
               and Lioma, Christina
               and Eickhoff, Carsten
               and N{\'e}v{\'e}ol, Aur{\'e}lie
               and Cappellato, Linda
               and Ferro, Nicola},
  title     = {Overview of CheckThat! 2020: Automatic Identification and Verification of Claims in Social Media},
  booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {215--236},
  abstract  = {We present an overview of the third edition of the CheckThat!  Lab at CLEF 2020. The lab featured five tasks in two different languages: English and Arabic. The first four tasks compose the full pipeline of claim verification in social media: Task 1 on check-worthiness estimation, Task 2 on retrieving previously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on claim verification. The lab is completed with Task 5 on check-worthiness estimation in political debates and speeches. A total of 67 teams registered to participate in the lab (up from 47 at CLEF 2019), and 23 of them actually submitted runs (compared to 14 at CLEF 2019). Most teams used deep neural networks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over the baselines on all tasks. Here we describe the tasks setup, the evaluation results, and a summary of the approaches used by the participants, and we discuss some lessons learned. Last but not least, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research in the important tasks of check-worthiness estimation and automatic claim verification.},
  isbn      = {978-3-030-58219-7}
}

@article{WisdomOfCrowds_Galton,
  title   = {Vox populi (the wisdom of crowds)},
  author  = {Galton, Francis},
  journal = {Nature},
  volume  = {75},
  number  = {7},
  pages   = {450--451},
  year    = {1907}
}

@article{ScalingUpFactChecking_Allen,
  author   = {Jennifer Allen  and Antonio A. Arechar  and Gordon Pennycook  and David G. Rand },
  title    = {Scaling up fact-checking using the wisdom of crowds},
  journal  = {Science Advances},
  volume   = {7},
  number   = {36},
  pages    = {eabf4393},
  year     = {2021},
  doi      = {10.1126/sciadv.abf4393},
  url      = {https://www.science.org/doi/abs/10.1126/sciadv.abf4393},
  eprint   = {https://www.science.org/doi/pdf/10.1126/sciadv.abf4393},
  abstract = {When rating articles’ accuracy, a small politically balanced crowd of laypeople yields high agreement with fact-checkers. Professional fact-checking, a prominent approach to combating misinformation, does not scale easily. Furthermore, some distrust fact-checkers because of alleged liberal bias. We explore a solution to these problems: using politically balanced groups of laypeople to identify misinformation at scale. Examining 207 news articles flagged for fact-checking by Facebook algorithms, we compare accuracy ratings of three professional fact-checkers who researched each article to those of 1128 Americans from Amazon Mechanical Turk who rated each article’s headline and lede. The average ratings of small, politically balanced crowds of laypeople (i) correlate with the average fact-checker ratings as well as the fact-checkers’ ratings correlate with each other and (ii) predict whether the majority of fact-checkers rated a headline as “true” with high accuracy. Furthermore, cognitive reflection, political knowledge, and Democratic Party preference are positively related to agreement with fact-checkers, and identifying each headline’s publisher leads to a small increase in agreement with fact-checkers.}
}

@article{ASurveyOnAutomatedFactChecking_Guo,
  author   = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  title    = {{A Survey on Automated Fact-Checking}},
  journal  = {Transactions of the Association for Computational Linguistics},
  volume   = {10},
  pages    = {178-206},
  year     = {2022},
  month    = {02},
  abstract = {{Fact-checking has become increasingly important due to the speed with which both
              information and misinformation can spread in the modern media ecosystem.
              Therefore, researchers have been exploring how fact-checking can be automated,
              using techniques based on natural language processing, machine learning,
              knowledge representation, and databases to automatically predict the veracity of
              claims. In this paper, we survey automated fact-checking stemming from natural
              language processing, and discuss its connections to related tasks and
              disciplines. In this process, we present an overview of existing datasets and
              models, aiming to unify the various definitions given and identify common
              concepts. Finally, we highlight challenges for future research.}},
  issn     = {2307-387X},
  doi      = {10.1162/tacl_a_00454},
  url      = {https://doi.org/10.1162/tacl\_a\_00454},
  eprint   = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00454/1987018/tacl\_a\_00454.pdf}
}

@inproceedings{DetectingCheckWorthyClaims_Hassan,
  author    = {Hassan, Naeemul and Li, Chengkai and Tremayne, Mark},
  title     = {Detecting Check-Worthy Factual Claims in Presidential Debates},
  year      = {2015},
  isbn      = {9781450337946},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2806416.2806652},
  doi       = {10.1145/2806416.2806652},
  abstract  = {Public figures such as politicians make claims about "facts" all the time. Journalists and citizens spend a good amount of time checking the veracity of such claims. Toward automatic fact checking, we developed tools to find check-worthy factual claims from natural language sentences. Specifically, we prepared a U.S. presidential debate dataset and built classification models to distinguish check-worthy factual claims from non-factual claims and unimportant factual claims. We also identified the most-effective features based on their impact on the classification models' accuracy.},
  booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
  pages     = {1835–1838},
  numpages  = {4},
  keywords  = {fact checking, computational journalism, text classification},
  location  = {Melbourne, Australia},
  series    = {CIKM '15}
}

@inproceedings{FEVER_Thorne,
  title     = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
  author    = {Thorne, James  and
               Vlachos, Andreas  and
               Christodoulopoulos, Christos  and
               Mittal, Arpit},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-1074},
  doi       = {10.18653/v1/N18-1074},
  pages     = {809--819},
  abstract  = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.}
}

@article{UnsupervisedNamedEntityExtraction_Etzioni,
  title    = {Unsupervised named-entity extraction from the Web: An experimental study},
  journal  = {Artificial Intelligence},
  volume   = {165},
  number   = {1},
  pages    = {91-134},
  year     = {2005},
  issn     = {0004-3702},
  doi      = {https://doi.org/10.1016/j.artint.2005.03.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370205000366},
  author   = {Oren Etzioni and Michael Cafarella and Doug Downey and Ana-Maria Popescu and Tal Shaked and Stephen Soderland and Daniel S. Weld and Alexander Yates},
  keywords = {Information Extraction, Pointwise mutual information, Unsupervised, Question answering},
  abstract = {The KnowItAll system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an unsupervised, domain-independent, and scalable manner. The paper presents an overview of KnowItAll's novel architecture and design principles, emphasizing its distinctive ability to extract information without any hand-labeled training examples. In its first major run, KnowItAll extracted over 50,000 class instances, but suggested a challenge: How can we improve KnowItAll's recall and extraction rate without sacrificing precision? This paper presents three distinct ways to address this challenge and evaluates their performance. Pattern Learning learns domain-specific extraction rules, which enable additional extractions. Subclass Extraction automatically identifies sub-classes in order to boost recall (e.g., “chemist” and “biologist” are identified as sub-classes of “scientist”). List Extraction locates lists of class instances, learns a “wrapper” for each list, and extracts elements of each list. Since each method bootstraps from KnowItAll's domain-independent methods, the methods also obviate hand-labeled training examples. The paper reports on experiments, focused on building lists of named entities, that measure the relative efficacy of each method and demonstrate their synergy. In concert, our methods gave KnowItAll a 4-fold to 8-fold increase in recall at precision of 0.90, and discovered over 10,000 cities missing from the Tipster Gazetteer.}
}

@inproceedings{WebBasedStatisticalFactChecking_Magdy,
  author    = {Magdy, Amr and Wanas, Nayer},
  title     = {Web-Based Statistical Fact Checking of Textual Documents},
  year      = {2010},
  isbn      = {9781450303866},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1871985.1872002},
  doi       = {10.1145/1871985.1872002},
  abstract  = {User generated content has been growing tremendously in recent years. This content reflects the interests and the diversity of online users. In turn, the diversity among internet users is also reflected in the quality of the content being published online. This increases the need to develop means to gauge the support available for content posted online. In this work, we aim to make use of the web-content to calculate a statistical support score for textual documents. In the proposed algorithm, phrases representing key facts are extracted to construct basic elements of the document. Search is used thereon to validate the support available for these elements online, leading to assigning an overall score for each document. Experimental results have shown a difference between the score distribution of factual news data and false facts data. This indicates that the approach seems to be a promising seed for distinguishing different articles based on the content.},
  booktitle = {Proceedings of the 2nd International Workshop on Search and Mining User-Generated Contents},
  pages     = {103–110},
  numpages  = {8},
  keywords  = {content filtering, statistical fact analysis, web-based assessment},
  location  = {Toronto, ON, Canada},
  series    = {SMUC '10}
}

@inproceedings{SemanticFakeNewsDetection_Bracsoveanu,
  author    = {Bra{\c{s}}oveanu, Adrian M. P.
               and Andonie, R{\u{a}}zvan},
  editor    = {Rojas, Ignacio
               and Joya, Gonzalo
               and Catala, Andreu},
  title     = {Semantic Fake News Detection: A Machine Learning Perspective},
  booktitle = {Advances in Computational Intelligence},
  year      = {2019},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {656--667},
  abstract  = {Fake news detection is a difficult problem due to the nuances of language. Understanding the reasoning behind certain fake items implies inferring a lot of details about the various actors involved. We believe that the solution to this problem should be a hybrid one, combining machine learning, semantics and natural language processing. We introduce a new semantic fake news detection method built around relational features like sentiment, entities or facts extracted directly from text. Our experiments show that by adding semantic features the accuracy of fake news classification improves significantly.},
  isbn      = {978-3-030-20521-8}
}

@inproceedings{DBPedia_Auer,
  author    = {Auer, S\"{o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  title     = {DBpedia: A Nucleus for a Web of Open Data},
  year      = {2007},
  isbn      = {3540762973},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  abstract  = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human-andmachine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  booktitle = {Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference},
  pages     = {722–735},
  numpages  = {14},
  location  = {Busan, Korea},
  series    = {ISWC'07/ASWC'07}
}