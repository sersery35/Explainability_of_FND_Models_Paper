@article{ThePsycologyOfFakeNews_Pennycook,
  title    = {The Psychology of Fake News},
  journal  = {Trends in Cognitive Sciences},
  volume   = {25},
  number   = {5},
  pages    = {388-402},
  year     = {2021},
  issn     = {1364-6613},
  doi      = {https://doi.org/10.1016/j.tics.2021.02.007},
  url      = {https://www.sciencedirect.com/science/article/pii/S1364661321000516},
  author   = {Gordon Pennycook and David G. Rand},
  keywords = {fake news, misinformation, social media, news media, motivated reasoning, dual process theory, crowdsourcing, attention, information sharing},
  abstract = {We synthesize a burgeoning literature investigating why people believe and share false or highly misleading news online. Contrary to a common narrative whereby politics drives susceptibility to fake news, people are ‘better’ at discerning truth from falsehood (despite greater overall belief) when evaluating politically concordant news. Instead, poor truth discernment is associated with lack of careful reasoning and relevant knowledge, and the use of heuristics such as familiarity. Furthermore, there is a substantial disconnect between what people believe and what they share on social media. This dissociation is largely driven by inattention, more so than by purposeful sharing of misinformation. Thus, interventions can successfully nudge social media users to focus more on accuracy. Crowdsourced veracity ratings can also be leveraged to improve social media ranking algorithms.}
}

@article{TheScienceOfFakeNews_Lazer,
  author   = {David M. J. Lazer  and Matthew A. Baum  and Yochai Benkler  and Adam J. Berinsky  and Kelly M. Greenhill  and Filippo Menczer  and Miriam J. Metzger  and Brendan Nyhan  and Gordon Pennycook  and David Rothschild  and Michael Schudson  and Steven A. Sloman  and Cass R. Sunstein  and Emily A. Thorson  and Duncan J. Watts  and Jonathan L. Zittrain },
  title    = {The science of fake news},
  journal  = {Science},
  volume   = {359},
  number   = {6380},
  pages    = {1094-1096},
  year     = {2018},
  doi      = {10.1126/science.aao2998},
  url      = {https://www.science.org/doi/abs/10.1126/science.aao2998},
  eprint   = {https://www.science.org/doi/pdf/10.1126/science.aao2998},
  abstract = {Addressing fake news requires a multidisciplinary effort The rise of fake news highlights the erosion of long-standing institutional bulwarks against misinformation in the internet age. Concern over the problem is global. However, much remains unknown regarding the vulnerabilities of individuals, institutions, and society to manipulations by malicious actors. A new system of safeguards is needed. Below, we discuss extant social and computer science research regarding belief in fake news and the mechanisms by which it spreads. Fake news has a long history, but we focus on unanswered scientific questions raised by the proliferation of its most recent, politically oriented incarnation. Beyond selected references in the text, suggested further reading can be found in the supplementary materials.}
}

@article{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu,
  author     = {Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  title      = {Fake News Detection on Social Media: A Data Mining Perspective},
  year       = {2017},
  issue_date = {June 2017},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {19},
  number     = {1},
  issn       = {1931-0145},
  url        = {https://doi.org/10.1145/3137597.3137600},
  doi        = {10.1145/3137597.3137600},
  abstract   = {Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.},
  journal    = {SIGKDD Explor. Newsl.},
  month      = {sep},
  pages      = {22–36},
  numpages   = {15}
}

@article{FakeNewsAndThePublic_McKernon,
  title    = {Fake news and the public. How the press combats rumor, the market rigger, and the propagandist},
  issn     = {0017-789X},
  url      = {https://harpers.org/archive/1925/10/fake-news-and-the-public/},
  urldate  = {2018-05-04},
  journal  = {Harper's Magazine},
  author   = {McKernon, Edward},
  month    = oct,
  year     = {1925},
  keywords = {20th century, Journalism, Journalistic ethics, News agencies, Press and propaganda, Rumor, Stock exchanges and current events, United States}
}

 @article{ReutersInstituteDigitalNewsReport,
  title     = {Reuters Institute Digital News Report 2022},
  url       = {https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2022-06/Digital_News-Report_2022.pdf},
  journal   = {Digital News Report 2022},
  publisher = {Reuters Institute for the Study of Journalism},
  author    = {Newman, Nic and Fletcher, Richard and Robertson, Craig T. and Eddy, Kirsten and Nielsen, Rasmus Kleis},
  year      = {2022},
  month     = {06}
}

@inbook{USPresidentialElection2016,
  title     = {United States Presidential Election of 2016},
  booktitle = {Encyclopedia Britannica},
  author    = {Beckwith, David C},
  abstract  = {United States Presidential Election of 2016, American
               presidential election held on November 8, 2016, in which
               Republican Donald Trump lost the popular vote to Democrat
               Hillary Clinton by more than 2.8 million votes but won 30 states
               and the decisive electoral college with 304 electoral votes to
               Clinton's 227 and thus became the 45th president of the United
               States. The tumultuous, abrasive 2016 campaign defied
               established political norms. Clinton's campaign featured
               superior organization and fund-raising---and almost every
               election-eve poll pointed to a comfortable victory for her---but
               Trump's anti-Washington appeal to white working-class voters
               outside major cities in pivotal manufacturing states},
  month     = {10},
  year      = {2021}
}

@misc{FakeNewsNet_Shu,
  doi       = {10.48550/ARXIV.1809.01286},
  url       = {https://arxiv.org/abs/1809.01286},
  author    = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  keywords  = {Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {FakeNewsNet: A Data Repository with News Content, Social Context and Spatialtemporal Information for Studying Fake News on Social Media},
  publisher = {arXiv},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{LiarLiarPantsOnFire_Wang,
  author     = {William Yang Wang},
  title      = {"Liar, Liar Pants on Fire": {A} New Benchmark Dataset for Fake News
                Detection},
  journal    = {CoRR},
  volume     = {abs/1705.00648},
  year       = {2017},
  url        = {http://arxiv.org/abs/1705.00648},
  eprinttype = {arXiv},
  eprint     = {1705.00648},
  timestamp  = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Wang17j.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{FakeNewsDetectionUsingGeometricDeepLearning_Monti,
  author     = {Federico Monti and
                Fabrizio Frasca and
                Davide Eynard and
                Damon Mannion and
                Michael M. Bronstein},
  title      = {Fake News Detection on Social Media using Geometric Deep Learning},
  journal    = {CoRR},
  volume     = {abs/1902.06673},
  year       = {2019},
  url        = {http://arxiv.org/abs/1902.06673},
  eprinttype = {arXiv},
  eprint     = {1902.06673},
  timestamp  = {Tue, 21 May 2019 18:03:39 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1902-06673.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{UPFD_Dataset_Shu,
  author     = {Yingtong Dou and
                Kai Shu and
                Congying Xia and
                Philip S. Yu and
                Lichao Sun},
  title      = {User Preference-aware Fake News Detection},
  journal    = {CoRR},
  volume     = {abs/2104.12259},
  year       = {2021},
  url        = {https://arxiv.org/abs/2104.12259},
  eprinttype = {arXiv},
  eprint     = {2104.12259},
  timestamp  = {Mon, 03 May 2021 17:38:30 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-12259.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{GraphAttentionNetworks_Velickovic,
  doi       = {10.48550/ARXIV.1710.10903},
  url       = {https://arxiv.org/abs/1710.10903},
  author    = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  keywords  = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Graph Attention Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{FakeReddit_Nakamura,
  title     = {{F}akeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection},
  author    = {Nakamura, Kai  and
               Levy, Sharon  and
               Wang, William Yang},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.755},
  pages     = {6149--6157},
  abstract  = {Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present Fakeddit, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to Fakeddit.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}

@article{SomeLikeItHoaxDataset_Tacchini,
  doi       = {10.48550/ARXIV.1704.07506},
  url       = {https://arxiv.org/abs/1704.07506},
  author    = {Tacchini, Eugenio and Ballarin, Gabriele and Della Vedova, Marco L. and Moret, Stefano and de Alfaro, Luca},
  keywords  = {Machine Learning (cs.LG), Human-Computer Interaction (cs.HC), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Some Like it Hoax: Automated Fake News Detection in Social Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@paper{BuzzfaceDataset_Santia,
  author     = {Giovanni Santia and Jake Williams},
  title      = {BuzzFace: A News Veracity Dataset with Facebook User Commentary and Egos},
  conference = {International AAAI Conference on Web and Social Media},
  year       = {2018},
  keywords   = {Facebook; news veracity; Disqus; social bots},
  abstract   = {Veracity assessment of news and social bot detection have become two of the most pressing issues for social media platforms, yet current gold-standard data are limited. This paper presents a leap forward in the development of a sizeable and feature rich gold-standard dataset. The dataset was built by using a collection of news items posted to Facebook by nine news outlets during September 2016, which were annotated for veracity by BuzzFeed. These articles were refined beyond binary annotation to the four categories: mostly true, mostly false, mixture of true and false, and no factual content. Our contribution integrates data on Facebook comments and reactions publicly available on the platform’s Graph API, and provides tailored tools for accessing news article web content. The features of the accessed articles include body text, images, links, Facebook plugin comments, Disqus plugin comments, and embedded tweets. Embedded tweets provide a potent possible avenue for expansion across social media platforms. Upon development, this utility yielded over 1.6 million text items, making it over 400 times larger than the current gold-standard. The resulting dataset—BuzzFace—is presently the most extensive created, and allows for more robust machine learning applications to news veracity assessment and social bot detection than ever before.},
  url        = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17825/17046}
}

 @misc{NewsConsumptionAcrossSocialMedia_pewresearch,
  title     = {News consumption across social media in 2021},
  url       = {https://www.pewresearch.org/journalism/2021/09/20/news-consumption-across-social-media-in-2021/},
  journal   = {Pew Research Center's Journalism Project},
  publisher = {Pew Research Center},
  author    = {Walker, Mason and Matsa, Katerina Eva},
  year      = {2021},
  month     = {Sep}
}

@article{GetBackYouDontKnowMeLikeThat_Hannak,
  title        = {Get Back! You Don’t Know Me Like That: The Social Mediation of Fact Checking Interventions in Twitter Conversations},
  volume       = {8},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/14555},
  abstractnote = { &lt;p&gt; &lt;span&gt;The prevalence of misinformation within social media and online communities can undermine public security and distract attention from important issues. Fact-checking interventions, in which users cite fact-checking websites such as Snopes.com and FactCheck.org, are a strategy users can employ to refute false claims made by their peers. While laboratory research suggests such interventions are not effective in persuading people to abandon false ideas, little work considers how such interventions are actually deployed in real-world conversations. Using approximately 1,600 interventions observed on Twitter between 2012 and 2013, we examine the contexts and consequences of fact-checking interventions.W&lt;/span&gt;&lt;span&gt;e focus in particular on the social relationship between the individual who issues the fact-check and the individual whose facts are challenged. Our results indicate that though fact-checking interventions are most commonly issued by strangers, they are more likely to draw user attention and responses when they come from friends. Finally, we discuss implications for designing more effective interventions against misinformation.&lt;/span&gt; &lt;/p&gt; },
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Hannak, Aniko and Margolin, Drew and Keegan, Brian and Weber, Ingmar},
  year         = {2014},
  month        = {May},
  pages        = {187-196}
}

@misc{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han,
  doi       = {10.48550/ARXIV.2007.03316},
  url       = {https://arxiv.org/abs/2007.03316},
  author    = {Han, Yi and Karunasekera, Shanika and Leckie, Christopher},
  keywords  = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Graph Neural Networks with Continual Learning for Fake News Detection from Social Media},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{RumorDetectionBidirectionalGraphConvolutionalNetworks_Bian,
  doi       = {10.48550/ARXIV.2001.06362},
  url       = {https://arxiv.org/abs/2001.06362},
  author    = {Bian, Tian and Xiao, Xi and Xu, Tingyang and Zhao, Peilin and Huang, Wenbing and Rong, Yu and Huang, Junzhou},
  keywords  = {Social and Information Networks (cs.SI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{SAFEFND_Zhou,
  doi       = {10.48550/ARXIV.2003.04981},
  url       = {https://arxiv.org/abs/2003.04981},
  author    = {Zhou, Xinyi and Wu, Jindi and Zafarani, Reza},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {SAFE: Similarity-Aware Multi-Modal Fake News Detection},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{XAIConceptsTaxonomies_Arrieta,
  title    = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  journal  = {Information Fusion},
  volume   = {58},
  pages    = {82-115},
  year     = {2020},
  issn     = {1566-2535},
  doi      = {https://doi.org/10.1016/j.inffus.2019.12.012},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  author   = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}

@book{InterpretableMachineLearning_Molnar,
  title    = {Interpretable Machine Learning},
  author   = {Christoph Molnar},
  year     = {2022},
  subtitle = {A Guide for Making Black Box Models Explainable},
  edition  = {2},
  url      = {https://christophm.github.io/interpretable-ml-book}
}

@misc{TowardsARigorousScienceML_Velez,
  doi       = {10.48550/ARXIV.1702.08608},
  url       = {https://arxiv.org/abs/1702.08608},
  author    = {Doshi-Velez, Finale and Kim, Been},
  keywords  = {Machine Learning (stat.ML), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Towards A Rigorous Science of Interpretable Machine Learning},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{CanWeOpenTheBlackBoxOfAI_Castelvecchi,
  author  = {Castelvecchi, Davide},
  year    = {2016},
  month   = {10},
  pages   = {20-23},
  title   = {Can we open the black box of AI?},
  volume  = {538},
  journal = {Nature},
  doi     = {10.1038/538020a}
}

@article{XAI_Gunning,
  title        = {DARPA’s Explainable Artificial Intelligence (XAI) Program},
  volume       = {40},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2850},
  doi          = {10.1609/aimag.v40i2.2850},
  abstractnote = {&lt;p&gt;Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.&lt;/p&gt;},
  number       = {2},
  journal      = {AI Magazine},
  author       = {Gunning, David and Aha, David},
  year         = {2019},
  month        = {Jun.},
  pages        = {44-58}
}

@misc{TheMythosOfModelInterpretability_Lipton,
  doi       = {10.48550/ARXIV.1606.03490},
  url       = {https://arxiv.org/abs/1606.03490},
  author    = {Lipton, Zachary C.},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {The Mythos of Model Interpretability},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{SlaveToTheAlgorithm_EdwardsVeale,
  title   = {Slave to the Algorithm? Why a 'Right to an Explanation' Is Probably Not the Remedy You Are Looking For},
  author  = {Lilian Edwards and Michael Veale},
  journal = {Duke law and technology review},
  year    = {2017},
  volume  = {16},
  pages   = {18-84}
}

@article{HierarchicalPropagationNetworksForFND_Shu,
  title        = {Hierarchical Propagation Networks for Fake News Detection: Investigation and Exploitation},
  volume       = {14},
  url          = {https://ojs.aaai.org/index.php/ICWSM/article/view/7329},
  abstractnote = {&lt;p&gt;Consuming news from social media is becoming increasingly popular. However, social media also enables the wide dissemination of &lt;em&gt;fake news&lt;/em&gt;. Because of the detrimental effects of fake news, fake news detection has attracted increasing attention. However, the performance of detecting fake news only from news content is generally limited as fake news pieces are written to mimic true news. In the real world, news pieces spread through &lt;em&gt;propagation networks&lt;/em&gt; on social media. The news propagation networks usually involve multi-levels. In this paper, we study the challenging problem of &lt;em&gt;investigating&lt;/em&gt; and &lt;em&gt;exploiting&lt;/em&gt; news hierarchical propagation network on social media for fake news detection.&lt;/p&gt;&lt;p&gt;In an attempt to understand the correlations between news propagation networks and fake news, first, we build hierarchical propagation networks for fake news and true news pieces; second, we perform a comparative analysis of the propagation network features from structural, temporal, and linguistic perspectives between fake and real news, which demonstrates the potential of utilizing these features to detect fake news; third, we show the effectiveness of these propagation network features for fake news detection. We further validate the effectiveness of these features from feature importance analysis. We conduct extensive experiments on real-world datasets and demonstrate the proposed features can significantly outperform state-of-the-art fake news detection methods by at least 1.7% with an average F1&gt;0.84. Altogether, this work presents a data-driven view of hierarchical propagation network and fake news and paves the way towards a healthier online news ecosystem.&lt;/p&gt;},
  number       = {1},
  journal      = {Proceedings of the International AAAI Conference on Web and Social Media},
  author       = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Liu, Huan},
  year         = {2020},
  month        = {May},
  pages        = {626-637}
}