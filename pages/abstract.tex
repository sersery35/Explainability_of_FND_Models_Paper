\chapter{\abstractname}

%TODO: Abstract

Fake news is becoming a more significant issue as the mainstream news source becomes social media. Social media is capable of disseminating information at a rapid pace, which leads to widespread fake news in a matter of hours. The consequences of sharing misleading information can be destructive at many levels. This is why fake news, rumor, deception, and other aspects of false information are being studied to be automatically detected. Many studies focus on different aspects, creating a variety of perspectives for automated detection. However, this is not an easy and straightforward task. There exist many challenges considering fake news, from human psychology to language modeling. Humans are not good lie detectors and can easily be fooled by misdirections, untrustworthy sources, or social status. Therefore, we need objective systems that can tell the difference.\\
Nevertheless, languages have many features which need to be utilized efficiently by automated detection systems to distinguish real news from fake news. As we will discuss, even though news content is important, social context also proves to be essential in detecting fake news.\\
This thesis investigates two types of fake news detection models: a news content-based model and a model that utilizes both news content and social context called a mixed model or mixed approach.  These models were developed in other studies. We reproduce their work using their datasets and attempt to explain their models. We investigate the explainability of two kinds of neural networks, an inductive graph neural network and, a Transformer model. We explain the Transformer model with the partition explainer from the SHAP framework. The mixed model is explained by GNNExplainer, a recent and only study that attempts to explain the graph neural networks and provide the library for it. We report our findings as well as improvement suggestions for GNNExplainer.\\
From what we gathered from our experiments, we propose the adoption of input sensitivity analysis along with explanation techniques to discover unintended weaknesses of  the model. We also draw attention to the importance of interpretable and understandable explanations and suggest better visualization techniques where necessary.
