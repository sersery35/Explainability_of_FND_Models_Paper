% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Background and Related Work}\label{chapter:background}

We explain two research fields that create the bedrock of this thesis, namely, fake news detection and
explainable artificial intelligence. Both areas provide the foundation of tools used in this work. The first
provides the mechanisms and approaches to detect fake news, and the second offers a suite of techniques to interpret these mechanisms and strategies.\\
Initially, in \ref{sec:fakeNewsDetection}, we discuss societal challenges, the characteristics, and the history of fake news. Then we talk about the detection methods that were developed over the years. After showing the challenges of creating FND models, we conclude the first section with SOTA FND models.\\
After fake news detection, in \ref{sec:explainableArtificialIntelligence}, we first examine when XAI is necessary and its importance. Then, we define the suite of XAI and its goals, and finally, we determine the suite that aims to satisfy these goals.

\section{Fake News Detection}
\label{sec:fakeNewsDetection}
In the past decade, social media has become a place where anyone can share information. Although fast, free, and easy to access, obtaining real news from social media can be difficult, and one should do so at their own risk and always check the facts~\parencite{SocialMediaAndFakeNewsIn2016Election_Allcott,TheScienceOfFakeNews_Lazer}. Nevertheless, the news stream never ends; thus, the need to verify the credibility of news using automated systems arises. To address this necessity, the number of studies involving \emph{Fake News} or \emph{Fake News Detection} has dramatically increased in the last decade (Fig. \ref{fig:FN_vs_FND_Publications}).
\begin{figure}
    \centering
    \includegraphics[scale=0.5]{FN_vs_FND_Publications}
    \caption[Fake news and fake news detection publications by year.]{Total number of publications that include (1) \emph{Fake News} (blue) and (2) \emph{Fake News Detection} (orange) publications by year. Note that the publications for 2022 are not complete at the time of writing. Source: Scopus; Search Arguments: (1) TITLE-ABS-KEY("fake news*") PUBYEAR AFT 2014 (2) TITLE-ABS-KEY("fake news detection")}\label{fig:FN_vs_FND_Publications}
\end{figure}\\
In \ref{subsec:fakeNewsDetection_fakeNews}, we briefly present the history of fake news and look at studies that display the impact of fake news on society. In this section, we also define the terms fake news, disinformation, and misinformation. \\
In \ref{subsec:fakeNewsDetection_FoundationsOfFakeNews}, we explore social sciences and human psychology, delivering insights into why humans fall for or tend to believe fake news. Furthermore, we draw some insights from the social, technical, and data-oriented foundations of fake news.\\
We then list the available datasets used in FND and deliberate their advantages and disadvantages in \ref{subsec:FakeNewsDetection_Characterization}. Finally, in \ref{subsec:FakeNewsDetection_Characterization}, we summarize the evolution of detection algorithms, then we classify FND algorithms according to their input data type and focus.

\subsection{Fake News}
\label{subsec:fakeNewsDetection_fakeNews}
Throughout history, various forms of widespread misinformation have been recorded. In the thirteenth
century BC, Rameses the Great decorated his temples with paintings that tell stories of victory in the Battle
of Kadesh. However, the treaty between the two sides reveals that the battle's outcome was a stalemate~\parencite{HistorysGreatestLies_Weir}. Later in history, just after the printing press was invented in 1439, the circulation of fake news began. One of history's most famous examples of fake news is the “Great Moon Hoax”~\parencite{TheGreatMoonHoax_Foster}. In 1835, The Sun newspaper of New York published articles about an astronomer and a fictional colleague who had observed life on the moon. It turns out that these fictionalized articles brought them new customers and almost no backlash after the newspaper admitted that the articles mentioned earlier were a hoax~\parencite{TheLongAndBrutalHistoryOfFakeNews_Soll}.\\
In order to highlight the difference, using the definitions from~\parencite{ThePsycologyOfFakeNews_Pennycook}, we formally introduce the terms disinformation and misinformation as follows.
\begin{definition}[\emph{Disinformation}]
    "\emph{Information that is false or inaccurate and that was created with a deliberate intention to mislead people.}"~\parencite{ThePsycologyOfFakeNews_Pennycook}
\end{definition}
\begin{definition}[\emph{Misinformation}]
    "\emph{Information that is false, inaccurate, or misleading. Unlike disinformation, misinformation does not necessarily need to be created deliberately to mislead.}"~\parencite{ThePsycologyOfFakeNews_Pennycook}
\end{definition}
There is no fixed definition for fake news. Thus, we elaborate on the definitions of fake news. A limited definition is "news articles that are intentionally and verifiably false"~\parencite{SocialMediaAndFakeNewsIn2016Election_Allcott}. This definition stresses authenticity and intent. The inclusion of false information that can be confirmed refers to authenticity. On the other hand, intent refers to the deceitful intention to delude news consumers~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. This definition is widely used in other studies~\parencite{AutomaticDeceptionDetection_Conroy, TheFakeNewsSpreadingPlague_Mustafaraj, FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}.\\
Furthermore, recent social sciences studies~\parencite{TheScienceOfFakeNews_Lazer} define fake news as "fabricated information that mimics news media content in form but not in organizational process or intent". Similarly, this definition covers authenticity and intent; additionally, it includes the organizational process. More general definitions for fake news consider satire news as fake news due to the inclusion of false information even though satire news aim to entertain and inherently reveals its deception to the consumer~\parencite{WhenFakeNewsBecomesReal_Balmas, TheImpactOfRealNewsAboutFakeNews_Brewer, NewsVerificationByExploitingConflictingSocialViewpoints_Jin, FakeNewsOrTruthUsingSatiricalCues_Rubin}. Further definitions include hoaxes, satires, and blatant
fabrications~\parencite{DeceptionDetectionForFakeNews3TypesOfFakeNews_Rubin}
In this thesis, we are not interested in the organizational process and do not consider conspiracy theories~\parencite{ConspiracyTheories_Sunstein}, superstitions~\parencite{Superstition_Lindeman}, rumors~\parencite{RumorsAndHealthCareReform_Berinsky}, satire, or hoaxes. Therefore, we use the limited definition from~\parencite{SocialMediaAndFakeNewsIn2016Election_Allcott} and formally introduce it.
\begin{definition}[\emph{Fake News}]
    "\emph{News articles that are intentionally and verifiably false.}"~\parencite{SocialMediaAndFakeNewsIn2016Election_Allcott}
\end{definition}
Fake news can lead to disastrous situations, such as crashes in stock markets, resulting in millions of dollars of loss. For example, Dow Jones industrial average went down like a bullet (see Fig.~\ref{fig:MarketReactionToFakeTweet}) after a tweet about an explosion injuring President Obama went out due to a hack~\parencite{MarketQuaversAfterFakeAPTweet_ElBoghdady}.
\begin{figure}
    \centering
    \includegraphics[scale=0.33]{MarketReactionToFakeTweet.png}
    \caption[Market reaction to the fake tweet.]{The market's reaction to the fake tweet. The sharp decline caused by a single tweet. Figure obtained from~\parencite{MarketQuaversAfterFakeAPTweet_ElBoghdady}.}\label{fig:MarketReactionToFakeTweet}
\end{figure}\\
The detrimental impacts of fake news further extend to societal issues. During the campaigns for the U.S: presidential elections in 2016, fake news pieces became very believable and widespread. This led to a disastrous event in which a man, convinced by what he read on social media about a pizzeria trafficking humans, went on a shooting spree in that pizzeria. Later named Pizzagate~\parencite{Pizzagate_Fisher}, this incident illustrates the deadly impact of fake news. In fact, fake news can even affect presidential elections~\parencite{SocialMediaAndFakeNewsIn2016Election_Allcott, TrumpWonBecauseOfFacebook_Read}.\\
Recent history exhibits that some fake news spreads like wildfires through social media. Evidence shows that the most popular fake news stories were more widely shared than the most popular mainstream news stories~\parencite{Buzzfeed_FakeNewsOutperformRealNews_Silverman}.\\
Digital News Report 2022~\parencite{ReutersInstituteDigitalNewsReport} shares in its key findings that trust in the news is 42\% globally, the highest (69\%) in Finland, and the lowest (26\%) in the U.S.A. Additionally, the same study shows that in early 2022, in the week of the survey, between 45\% and 55\% of the surveyed social media consumers worldwide witnessed false or misleading information about COVID-19. The same study also reports the appearance of fake news in politics was between 34\% and 51\%, and between 9\% and 48\% for fake news about celebrities, global warming, and immigration~\parencite{StatistaUsageOfSocialMedia_Watson}.
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{TotalFacebookEngagementsForTop20ElectionStories}
    \caption[Total Facebook engagements for top 20 election stories.]{The rising engagement for fake news stories observed after May-July, just before Presidential Elections. Figure obtained from~\parencite{Buzzfeed_FakeNewsOutperformRealNews_Silverman}.}\label{fig:TotalFacebookEngagementsForTop20ElectionStories}
\end{figure}

\subsection{Foundations of Fake News Detection}
\label{subsec:fakeNewsDetection_FoundationsOfFakeNews}
In this subsection, we discuss the psychological and social foundations of fake news to stress the importance
of human psychology, especially when accepting fake news as genuine and sharing it with others. Then we focus on
the technical foundations, where we discuss how social media and technology have accelerated the diffusion of fake news.\\
\textbf{Psychological and Social Foundations.}  Understanding the difference between real and fake news is not an easy task
for a human. Two psychological theories, namely, \emph{naive realism} and \emph{confirmation bias}, examine why humans fall for fake news. The first refers to a person's disposition to believe that only their point of view is accurate, while people who believe otherwise are uninformed or biased~\parencite{NaiveRealism_Reed}. The second, often called selective exposure, is the proclivity to prefer information that confirms existing views~\parencite{ConfirmationBias_Nickerson}.\\
Another reason for human fallacy in fake news is that once a misperception is formed, it becomes difficult to correct. In fact, it turns out that correcting people leads them to believe false information more, especially
when given factual information that refutes their beliefs~\parencite{WhenCorrectionsFail_Nyhan}.\\
The prospect theory explains the human decision-making process as a mechanism based on maximizing relative gains and minimizing losses with respect to the current state~\parencite{ProspectTheory_Kahneman, AdvancesInProspectTheory_Kahneman}. This inherent inclination to get the highest reward also applies to social cases in which a person will seek social networks that provide them with social acceptance.~\citeauthor{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu} (\citeyear{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}) explain this connection using an economic game theory perspective. In an information ecosystem, the key players are defined as the \emph{publisher} and the \emph{consumer}. The authors describe utilities for the publishers and consumers. The utilities for the publishers are \emph{short-term} and \emph{long-term} utilities. Short-term utilities deal with maximizing profits, whereas long-term utilities are concerned with the publisher's reputation of news authenticity. For consumers, there are also two kinds of utilities: \emph{information} and \emph{psychology}. Information utility refers to getting correct and unbiased information. Psychology utility stands for reading news that complies with the consumer's prior ideas and social needs. In this game of news consumption process, both players aim to maximize their utility. Accordingly, if (i) short-term utilities dictate the overall utility of the publisher, (ii) psychological utilities prevail in the overall utility of the consumers, and (iii) an equilibrium is reached, then this leads to an information ecosystem where fake news can flourish~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. From this, we can infer that this equilibrium is reached when both parties display loss-averse strategies. For fake news publishers, loss-averse behavior maximizes profit in the short term. For fake news consumers, loss-averse decisions ensure psychological satisfaction and social safety.\\
Consequently, people with different views tend to form separate groups, which makes them feel safer, leading to the consumption and dissemination of information that agrees with their opinions. These behaviors are explained by social identity theory~\parencite{SocialIdentityTheory_Ashforth} and normative social influence~\parencite{NormativeSocialInfluence_Asch}. Two psychological factors play a crucial role here~\parencite{TheRussianFirehoseOfFalsehood_Paul}. The first, social credibility, is explained by a person’s tendency to recognize a source as credible when that source is deemed credible by other people. The second, called the frequency heuristic, is the acceptance of a news piece by repetitively being exposed to it. Collectively, these psychological phenomena are closely related to the well-known filter bubble~\parencite{TheFilterBubble_Pariser}, also called echo chamber, which is the formation of homogenous bubbles in which the users are people of similar ideologies and share similar ideas. Being isolated from different views, these users usually are inclined to have highly polarized opinions~\parencite{EchoChambers_Sunstein}. As a result, the main reason for misinformation dispersal turns out to be the echo chambers~\parencite{TheSpreadingOfMisinformationOnline_DelVicario}.\\
\textbf{Technical Foundations.} Social media's easy-to-use and connected nature give rise to more people selecting or even creating their own news source. Naturally, this gives way to more junk information echoing in a group of people on social media. As algorithms evolve to understand user preferences, social media platforms recommend similar people or groups to those in echo chambers. A recent study~\parencite{TheEffectOfPeopleRecommenderOnEchoChambers_Cinus}  shows that these recommenders can strengthen these echo chambers. They discuss that some of these recommenders contribute to the polarization on social media. In other words, people can convince themselves that any fake news is real by staying in their echo chambers. One main reason that some fake news spreads so rapidly on social media is the existence of malicious accounts. The account user can be an actual human or a social bot since creating accounts on social media is no cost and almost no effort. While many social bots provide valuable services, some were designed to harm, mislead, exploit, and manipulate social media discourse. Formally, a social bot is a social media account governed by an algorithm to fabricate content and interact with other users~\parencite{TheRiseOfSocialBots_Ferrara}. A more recent study from the same author shows that malicious social bots were heavily used in the 2016 U.S. Presidential Elections~\parencite{SocialBotsDistortThe2016USPresidentialElection_Bessi}. On the other hand, malicious accounts that are not bots, such as online trolls who aim to trigger negative emotions and humans that provoke people on social media to get an emotional response, contribute to the proliferation of fake news~\parencite{AnyoneCanBecomeATroll_Cheng}.\\
% Building upon these foundations, we draw some results for fake news to be considered when building a fake news detection model:
% \begin{enumerate}
%     \item \emph{Invasive}: Fake news are likely to appear on anyone’s feed if it spreads for a sufficient amount of time.
%     \item \emph{Hard to discern}: Fake news is fabricated in such a way that it resembles the authenticity of a real news source. This
%           indistinguishability leads to issues when working with news-content-based FND models.
%     \item \emph{The source is crucial}: The credibility of a news source is essential. We can use news from credible sources to teach the model
%           to distinguish genuine from fabricated.
%     \item \emph{Fake news has hot spots}: The echo chambers are invaluable examples when trying to understand the behaviors of fake news. We can
%           leverage this attribute and use social models, such as graphs, to successfully detect fake news.
%     \item \emph{Early detection is essential}: As discussed in psychological foundations, the volume of exposure to a piece of fake news can significantly affect one’s opinions, thus leading to more misinformed individuals.
% \end{enumerate}
\textbf{Data-Oriented Foundations.} We define features for news content and social context to represent the news pieces in a structured manner. First, we introduce attributes for news content~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}:
\begin{itemize}
    \item \emph{Source}: Publisher of the news piece.
    \item \emph{Headline}: Short title text that aims to catch the readers’ attention and describes the article's main topic.
    \item \emph{Body Text}: The main text piece that details the news story.
    \item \emph{Image/Video}: Part of the body content supplies visual input to articulate the story.
\end{itemize}
Using these attributes, we describe two types of features for news content:
\begin{description}
    \item{\emph{Linguistic-based features}:} The news content is heavily based on textual content. Thus, the first feature that belongs to this class is lexical features which make use of character and word level frequency information that can be obtained by the utilization of \emph{term frequency}-\emph{inverse term frequency} (TF-IDF)~\parencite{TF_Luhn, IDF_Jones} or bag-of-words (BoW). The second feature is based on syntactic features, which include sentence-level features that can be obtained via \emph{n-grams} and punctuation and \emph{parts-of-speech} (POS)~\parencite{POS_Daelemans} tagging. We can extend these features to domain-specific ones, such as external links and the number of graphs~\parencite{AStylometricInquiry_Potthast}.
    \item{\emph{Visual-based features}:} Particularly for fake news, the visual content is a vital tool for establishing
    belief~\parencite{VisualMisAndDisinformation_Viorela}. Hence, the features that reside in images and videos become significant. Fake images and videos that bring the fake story together are commonly used(e.g.,~\cite{PutinBehindBars_Harding, DeepFakeQueensSpeech_Sawer}). A recent study~\parencite{ExploitingMultiDomainVisualInformation_Qi} examines visual and statistical information for fake news detection to counteract the effects of misleading visual input. Visual features consist of clarity score, similarity distribution histogram, diversity score, and clustering score. Statistical features are listed
    as count, image ratio, multi-image ratio, etc.~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}.
\end{description}
Now, we define features for social context, which has recently drawn much attention from the research community~\parencite{BeyondNewsContents_Shu, HierarchicalPropagationNetworksForFND_Shu}. Overall, we will concern with three aspects of social context data: user-based, post-based, and network-based features.
\begin{description}
    \item{\emph{User-based}:} As mentioned in the Technical Foundations part of this subsection, fake news has various ways of disseminating, such as via echo chambers, malicious accounts, or bots. Therefore, analyzing user-based information can prove useful. We distinguish user-based features at the group and individual levels~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. Individual levels are extracted to deduce the credibility of each user by utilizing, for example, the number of followers and followees, the number of tweets authored by a user, or similar features~\parencite{InformationCredibilityOnTwiter_Castillo}. On the other hand, group-level user-based features are the general characteristics of groups of users related to the news~\parencite{AutomaticDetectionOfRumor_Yang}. Parallel to the social identity theory and normative social influence idea, the assumption is that the consumers of real and fake news about a topic tend to form different groups, which may lead to unique characteristics. Typical group-level features stem from individual-level features by obtaining the share of verified users and the average number of followers and followees~\parencite{DetectRumorsUsingTimeSeries_Ma}.
    \item{\emph{Post-based}:} Analysis of reactions by users can prove helpful when determining whether a news piece is real or not. For example, if a news piece is getting doubtful comments, this can help determine the news piece’s credibility. As such, post-based features are based on inferring the integrity of a news piece from three levels. Namely, post-level, group-level, and temporal-level~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. Post-level features can be embedding values for each post or take forms mentioned in linguistic-based features. For post-level features, we can also consider general approaches such as topic extraction (e.g., using latent Dirichlet allocation (LDA)~\parencite{LatentDirichletAllocation_Blei}), stance extraction, which provides information about users’ opinions (e.g., supports, opposes~\parencite{NewsVerificationByExploitingConflictingSocialViewpoints_Jin}), and finally credibility extraction, which deals with estimating the degree of trust for each post~\parencite{InformationCredibilityOnTwiter_Castillo}. Group-level post-based features collect feature values for all relevant posts and apply an operation to extract pooled information. When determining the credibility of news, group-level features proved to be helpful~\parencite{NewsVerificationByExploitingConflictingSocialViewpoints_Jin}. Temporal-level features deal with changes in post-level features over time. Typically, unsupervised learning methods, such as Recurrent Neural Networks (RNN) are employed to capture the changes over time~\parencite{DetectingRumorsFromMicroblogs_Ma}.
    \item{\emph{Network-based}:} As discussed in the Technical Foundations part, fake news pieces are likely to disseminate in echo chambers, which motivates the idea of a network-based approach. When represented as networks, the propagation behavior of fake news can be analyzed further, and patterns can be discovered~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. In literature, various types of networks exist. The most common ones are stance networks, occurrence networks, and friendship networks. Stance networks are constructed upon stance detections which is a part of sentiment analysis and deal with determining a user’s viewpoint using text and social data~\parencite{StanceClassificationAttention_Du}. Using all users’ stances, a network is built in which the nodes are the tweets relevant to the news piece and the edges represent the similarity of stances between nodes~\parencite{NewsVerificationByExploitingConflictingSocialViewpoints_Jin, SomeLikeItHoaxDataset_Tacchini}. On the other hand, occurrence networks leverage the frequency of mentions or replies about the same news piece~\parencite{ProminentFeaturesOfRumorPropagation_Kwon}. Friendship networks are based on the follower/followee relationship of users who share posts connected to the news piece. Derived from friendship networks, in the form of one of the datasets we use in our experiments~\parencite{UPFD_Dataset_Shu}, diffusion networks are designed to track the course of the dissemination of news~\parencite{ProminentFeaturesOfRumorPropagation_Kwon}. Briefly, a diffusion network consists of nodes characterizing users and diffusion paths representing user relationships and interactions. In detail, a diffusion path between two users $u_i$ and $u_j$ exists if and only if $u_j$ follows $u_i$, and $u_j$ shares a post about a news piece that $u_i$ has already shared a post about~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. It has been shown that characterizing these networks is possible~\parencite{ProminentFeaturesOfRumorPropagation_Kwon}. Approaches for these networks have gained traction recently, especially with some SOTA GNNs, e.g.,~\parencite{FakeNewsDetectionUsingGeometricDeepLearning_Monti}.
\end{description}
To conclude this subsection, we have covered psychological, social, technical, and data-oriented foundations in this section. We established that,  from different aspects, there are various reasons for the dissemination of fake news. Accordingly, we consider these reasons when building FND systems. In the next section, we discuss FND approaches and how they have evolved. Moreover, we characterize FND models and talk about each type of approach.

\subsection{Characterization of Fake News Detection}
\label{subsec:FakeNewsDetection_Characterization}
Fake news detection is as old as fake news itself. Before social media became a hub for news consumers, fact-checkers, i.e., fake news detectors, were only journalists and educated people. Following the source shift of the news from printed paper to online, then social media, the detection of fabricated news has become costly, cumbersome, and not as rewarding due to the endless stream of information and decreasing trust in journalism. Automatic detection for news thus became a necessity in our world~\parencite{NewsInAnOnlineWorld_Chen}.\\
We classify fake news detection models as \emph{news content models} and  \emph{social context models} (see Fig.~\ref{fig:FakeNewsDetectionModelsClassification}) and start with news content models by following the classification principles in~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}.\\
\begin{figure}
    \centering
    \includegraphics[scale=0.75]{FakeNewsDetectionModelsClassification.png}
    \caption[Characterization of fake news detection models.]{Characterization of Fake News Detection Models, Figure inspired by Figure 1 in~\citeauthor{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu} (\citeyear{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}).}
    \label{fig:FakeNewsDetectionModelsClassification}
\end{figure}
\textbf{News Content Models.} Based on news content and fact-checking methodologies, these models are the starting point of fake news detection. News content models are classified as knowledge-based and style-based. We first introduce style-based models as they are the initial approaches for FND.
\begin{description}
    \item{\emph{Style-based}:} Previous research in psychology has mainly focused on style-based approaches to detect \emph{manipulators} in the text. Particularly, deception detection techniques were popular and commonly developed in early works in criminology and psychology. We describe two different ways to approach style-based news content models, namely, \emph{Deception-oriented} and \emph{Objectivity-oriented}~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}.
    \begin{itemize}
        \item \emph{Deception-oriented}: The initial approaches for automated fake news detection focus on news context and stem from deception detection in language. The first study that focuses on deception detection in the language~\parencite{DieEntwicklungDerGerichtspsychologischen_Undeutsch} hypothesized that the truthfulness of the statement is more important than the integrity of the reporting person, and there exist definable and descriptive criteria that form a crucial mechanism for determining the truthfulness of statements. Even though this study is from experimental psychology, it stresses the feasibility of defining a set of rules that determine the truthfulness of a statement.\\
              An early study from criminology, Scientific Content Analysis (SCAN)~\parencite{SCAN_Sapir1987}, analyzes freely written statements.  In this process, SCAN claims to detect potential instances of deception in the text but cannot label a statement as a lie or truth. The subsequent study for SCAN~\parencite{SCAN_Smith2001} is the first known study that correlates linguistic features with deceptive behavior using high-stakes data. Similar to SCAN, the subsequent studies~\parencite{CommunicationUnderStress_Adams, LyingWords_Newman} that link linguistic features to deception classify the owner of the statement as a truth-teller or liar according to the frequency of deception indicators in the statement.\\
              Although it is more challenging to define a methodology for automated deception detection~\parencite{TheAccuracyConfidenceRelation_DePaulo}, early studies have shown that this task is achievable.~\citeauthor{AutomatingLinguisticsBasedCues_Zhou} (\citeyear{AutomatingLinguisticsBasedCues_Zhou}) create a structured approach using linguistic-based cues and draw attention to further studies for automating deception detection. Linguistic-based cues include passive voice, self reference, negative statements, generalizing terms, uncertainties, temporal information, spatial information, and affect. In this study, the authors extend linguistic-based cues with complexity, expressivity, informality, and content diversity. Instead of using humans as cue identifiers, the authors use \emph{Natural Language Processing} (NLP) techniques, namely an NLP tool called iSkim~\parencite{iSkim_Zhou}, to extract cues automatically. Another study also focuses on linguistic cue analysis. Employing the C4.5 algorithm~\parencite{C45_Salzberg} on a small dataset, the authors reach 60.72\% accuracy using 15-fold cross-validation.\\
              Similarly, in~\citeauthor{VerificatoinAndImplementationofLBDeceptionIndicators_Bachenko} (\citeyear{VerificatoinAndImplementationofLBDeceptionIndicators_Bachenko}), the authors developed a system for automatically identifying 275 truthful or deceitful statements with the use of verbal cues using the Classification and Regression Tree (CART)~\parencite{ClassificationRegressioniTrees_Breiman}. Additionally, the studies ~\parencite{OnLyingAndBeingLiedTo_Hancock, OnDeceptionAndDeceptionDetection_Rubin} make use of a relatively small dataset and analyze linguistic-based cues. Rubin’s series of  studies~\parencite{OnDeceptionAndDeceptionDetection_Rubin, IdentificationOfTruth_Rubin, TruthAndDeception_Rubin, TowardsNewsVerification_Rubin} adopt Rhetorical Structure Theory (RST) and Vector Space Modeling (VSM). The first captures the coherence of a story using functional relations among meaningful text units and delivers a hierarchical structure for each news story~\parencite{RST_William}. The second is a way to represent rhetorical relations in high-dimensional space. The authors utilized logistic regression as their classifier and reached 63\% accuracy.\\Furthermore,~\citeauthor{DetectingHoaxesFraudsAndDeception_Afroz} (\citeyear{DetectingHoaxesFraudsAndDeception_Afroz}) investigate stylistic deception and use lexical, syntactic, and content-specific features. Lexical features include both character- and word-based features. Syntactic features represent sentence-level style and include frequency of function words from LIWC~\parencite{LIWC2007_Pennebaker}, punctuation, and POS tagging in which a text is assigned its morphosyntactic category~\parencite{POS_Daelemans}. Lastly, content-specific features are keywords for a specific topic. For classification, the authors leveraged Support Vector Machines (SVM)~\parencite{SVM_Hearst}. More comprehensive and modern approaches, such as~\citeauthor{LiarLiarPantsOnFire_Wang} (\citeyear{LiarLiarPantsOnFire_Wang}) also leveraged the power of \emph{Convolutional Neural Networks} (CNNs) to determine the veracity of the news.
        \item \emph{Objectivity-oriented}: Objectivity-oriented news content models aim to detect indicators of objectivity decrease in the news content~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. These indicators are observed in the news content from misleading sources, such as hyperpartisan sources, which display highly polarized opinions in favor of or against a particular political party. Consequently, this polarized behavior motivates the fabrication of news that supports the sources’ political views or undermines the opposing political party. \emph{Hyperpartisan news} is a subtle form of fake news. It is defined as misleading coverage of events that did actually occur with a strong partisan bias~\parencite{FightingMisinformationOnSocialMedia_Pennycook}. Since the spread of hyperpartisan news can be detrimental, many approaches to detect hyperpartisanship in news articles have been developed. For instance, in~\citeauthor{AStylometricInquiry_Potthast} (\citeyear{AStylometricInquiry_Potthast}), the authors take a stylometric methodology that adopts 10 readability scores, and dictionary features based on the frequency of words. A competition for detecting hyperpartisan news~\parencite{SemEvalHyperpartisanNewsDetection_Kiesel} hosted several teams with various ideas, including the utilization of n-grams, word embeddings, stylometry, sentiment analysis, and many more approaches. The most popular method was the usage of embeddings, particularly the models that leveraged BERT~\parencite{BERT_Devlin}.\\ Also used for the dissemination of hyperpartisan news~\parencite{SemEvalHyperpartisanNewsDetection_Kiesel}, another form of fake news that is evaluated under this focus is \emph{yellow-journalism}. It utilizes clickbaits such as catchy headlines and images that invoke strong emotions, and aims to generate revenue~\parencite{ClickbaitDetectionUsingDL_Agrawal, ClickbaitAndTabloidStrategies_Dolors}. Studies that aim to detect click baits mainly focus on headlines. For example,~\citeauthor{DivingDeepIntoClickbaits_Rony} (\citeyear{DivingDeepIntoClickbaits_Rony})  construct a DNN in which they use distributed subword embeddings~\parencite{EnrichingWordVectorsWithSubwordInfo_Bojanowski, BagOfTricksForTextClassificatoin_Joulin} as features with an extension of the skip-gram model~\parencite{DistributedRepresentationsOfWords_Mikolov}.
    \end{itemize}
    \item{\emph{Knowledge-based}}: Being the most direct way of detecting fake news, these approaches make use of external fact-checkers to verify the claims in news content~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. Fact-checkers can be sophisticated algorithms, domain experts, or crowdsourcing methods that assess the truthfulness of a claim in a specific context~\parencite{FactChecking_Vlachos}. With growing consideration for fake news detection, automated fact-checking has drawn much attention, and notable efforts have been made in this area~\parencite{AutomatedFactChecking_Thorne, OverviewOfCheckThat_Barroncede}. We categorize knowledge-based news content models as \emph{expert-oriented}, \emph{crowdsourcing-oriented}, and \emph{computational-oriented}.
    \begin{itemize}
        \item \emph{Expert-oriented} approaches depend on domain experts who investigate the integrity of a news piece collecting relevant information and documents to decide the truthfulness of a claim~\parencite{ThePrincipleOfTruthOMeter_Holan}. Platforms like Politifact~\footnote{\url{https://www.politifact.com/}} and EUfactcheck~\footnote{\url{https://eufactcheck.eu/}} are examples of expert-oriented fact-checking for all news from various sources. These platforms label news in a range such that the label reflects the veracity of the news. A different approach for labeling is exercised by Snopes~\footnote{\url{https://www.snopes.com/}}, which extends the same logic of Politifact by including different aspects of fact-checking, such as "scam", "miscaptioned", and "outdated"~\parencite{FactCheckRatings_Snopes}. Recently replaced by an irrelevant magazine website, another instance was Gossipcop~\footnote{\url{https://web.archive.org/web/20190807002653/https://www.gossipcop.com/about/}}, which dealt with celebrity fact-checking and contributed to the creation of a fake news dataset~\parencite{FakeNewsNet_Shu}. Even though expert-based fact-checking is reliable, with the increasing magnitude of news stream and speed of spread, it is not scalable to fact-check every news piece by hand; thus, manual validation alone becomes insufficient~\parencite{ASurveyOnAutomatedFactChecking_Guo}.
        \item \emph{Crowdsourcing-oriented} fact-checking is powered by the wisdom of crowds~\parencite{WisdomOfCrowds_Galton}, and is a collection of annotations that are afterward aggregated to obtain an overall result indicating the veracity of the news. Unlike professional fact-checkers, who are in short supply, this approach is scalable given that the crowd contains enough educated people~\parencite{ScalingUpFactChecking_Allen}.  For instance, Twitter launched a program called Birdwatch, in which users can leave notes for tweets that they think contain misinformation~\parencite{BirdwatchOverview_Twitter}. Furthermore, this tool allows users to rate each other’s notes, leading to diverse perspectives~\parencite{DiversityOfPerspectives_Twitter}. Another example is from Meta, which uses a third party of crowdsourced fact-checkers called International Fact-Checking Network~\footnote{\url{https://www.poynter.org/ifcn/}} (IFCN)~\parencite{MetaFactCheckingProgram_Meta}.
        \item \emph{Computational-oriented} methods, unlike expert-oriented systems, are scalable and designed to automatically predict whether a claim is truthful. These systems are dependent on external sources. The studies that focus on this type of approach mainly try to solve two issues: (i) \emph{identifying check-worthy claims} and (ii) \emph{estimating the integrity of claims}~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. The first issue requires the extraction of factual claims from news content or other related textual content. For example,~\citeauthor{DetectingCheckWorthyClaims_Hassan} (\citeyear{DetectingCheckWorthyClaims_Hassan}) collect presidential debate transcripts, then label them into three classes with the help of crowdsourcing. The authors uncover interesting patterns in these transcripts using annotated data and supervised learning techniques. Another study that covers both issues uses Wikipedia information to generate factual claims, then check whether a given claim is truthful~\parencite{FEVER_Thorne}. Compared to the first one, the second issue requires the utilization of structured external sources. \emph{Open web} and structured \emph{knowledge graphs} are the two most prominent tools when tackling this issue. Open web tools analyze features like mutual information statistics~\parencite{UnsupervisedNamedEntityExtraction_Etzioni}, frequency, and web-based statistics~\parencite{WebBasedStatisticalFactChecking_Magdy}.  On the other hand, knowledge graphs are interconnected. One noteworthy example is ontologies such as DBPedia~\parencite{DBPedia_Auer}. Using DBPedia, one can define semantic relations and rules to infer whether a claim is correct~\parencite{SemanticFakeNewsDetection_Bracsoveanu}.
    \end{itemize}
\end{description}
\textbf{Social Context Models.} The interconnected design of social media can be leveraged by extracting user-based, post-based, and network-based features and utilizing these features to supplement news content models. Social context models exploit related user engagements for a news piece by capturing this external information from multiple angles. Two types of social context models are prominent: \emph{stance-based} and \emph{propagation-based}~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}.
\begin{itemize}
    \item \emph{Stance-based} approaches estimate the user’s stance toward a specific news topic. More formally, stance detection in social media deals with users’ viewpoints toward particular topics by means of various aspects related to users’ posts and characteristic traits~\parencite{StanceDetectionOnSocialMeda_Abeer}. The user’s stance information can be extracted either implicitly or explicitly. Implicit stances can be automatically obtained from social media posts with the help of NLP tools such as sentiment analysis~\parencite{StanceAndSentimentINTweets_Saif}. Explicit stances are easier to obtain since they are direct expressions of opinions or emotions. For example, “like” on Twitter or Facebook, “upvote” and “downvote” ratings on Reddit, and “like” and “dislike” on Youtube are explicit stances of users. Some Like it Hoax, a study by~\citeauthor{SomeLikeItHoaxDataset_Tacchini} (\citeyear{SomeLikeItHoaxDataset_Tacchini}) utilize explicit stances with logistic regression and harmonic boolean label crowdsourcing for classification on a dataset they curated from Facebook. In the stance classification process, the authors consider the likes and the issuer of likes for each post. They state that logistic regression comes short in this task since it cannot learn anything about posts without likes~\parencite{SomeLikeItHoaxDataset_Tacchini}. An early example of implicit stance detection~\parencite{StanceClassificationDialogicProps_Walker} leverages the dialogic relations between authors by constructing graphs that represent the interaction between authors. They show that this information can improve the performance of stance-detection models. A more detailed study~\parencite{StanceClassificationOnTwitterDebates_Addawood} investigates stance classification considering lexical, syntactic, twitter-specific, and argumentation feature types. Although some twitter-specific features can be considered explicit stances, such as whether the tweet is a retweet, or it contains the title of an article or a hashtag. Those features are later aggregated before it is fed to the classifier. The authors reach the highest F1 score using lexical and argumentation features. In literature, there are also implicit stance-based approaches that aim to detect the veracity of a news piece by exploring the relationship between a headline and the article~\parencite{ARetrospectiveAnalysisOfFNC_Hanselowski, StanceDetectionInFakeNews_Ghanem}.\\Another variation of stance-based detection is rumor detection. One example of a rumor detection model is a Bayes classifier that utilizes content-based, network-based, and twitter-specific meme features through \emph{Information Retrieval} (IR) techniques~\parencite{RumorHasIt_Qazvinian}.  In this study, the authors propose a general framework that leverages statistical models and maximizes a linear function of log-likelihood ratios to retrieve rumorous tweets. They show that the features they used contribute to their model's overall performance.
    \item \emph{Propagation-based} procedures are inspired by the assumption that the veracity of a news event is highly correlated with the credibilities of related social media posts. These models employ the interrelations of related social media posts to classify a news piece as truthful or not~\parencite{FakeNewsDetectionOnSocialMediaADataMiningPerspective_Shu}. Propagation-based models can be based on either \emph{homogeneous networks} or \emph{heterogeneous networks}. Homogeneous networks are built upon a single type of entity, such as a post or event. A study by~\citeauthor{NewsVerificationByExploitingConflictingSocialViewpoints_Jin} (\citeyear{NewsVerificationByExploitingConflictingSocialViewpoints_Jin}) creates homogeneous credibility networks for each topic which is extracted using an unbalanced version of the Joint Topic Viewpoint Model~\parencite{FindingAndArguingExpressions_Trabelsi}. These credibility networks consist of nodes as tweets and edges as links, defined by either supporting or opposing. On the other hand, heterogeneous networks can contain multiple types of entities, such as events, sub-events, posts, and comments. For example,~\citeauthor{NewsCredibilityEvaluationOnMicroblog_Jin} (\citeyear{NewsCredibilityEvaluationOnMicroblog_Jin}) builds a hierarchical propagation graph that contains events, sub-events, and messages. Using an iterative method, the authors provide a globally optimal solution for the graph optimization problem in the study.\\
\end{itemize}
\textbf{Mixed Approaches.} We examined two types of FND models, namely,  news content models and social context approaches. It is crucial to note that approaches are not necessarily purely news content or social context-based; they can be based on both. The models that use news content and social context are referred to as mixed approaches~\parencite{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han}. \emph{User Preference-Aware Fake News Dataset} (UPFD)~\parencite{UPFD_Dataset_Shu} is a dataset that houses two of these types. UPFD framework uses news content and social engagement information to construct a hierarchical tree. The node features are textual representations of various sources which will be discussed in~\ref{subsec:mixedApproaches_DatasetAndModel}. The best-performing model uses GraphSAGE~\parencite{GraphSAGE_Hamilton} as graph encoder and BERT~\parencite{BERT_Devlin} as the text encoder.\\
To summarize this section, we have introduced the history and definitions of fake news in subsection~\ref{subsec:fakeNewsDetection_fakeNews}. Then, we investigated the foundations of fake news and gave motivations for developing automated FND systems in subsection~\ref{subsec:fakeNewsDetection_FoundationsOfFakeNews}. Following that, we examined the evolution and characterized FND models in section~\ref{subsec:FakeNewsDetection_Characterization}. We discussed examples for each type of model and briefly summarized their approaches. We also introduced one of the datasets~\parencite{UPFD_Dataset_Shu} and models~\parencite{GraphSAGE_Hamilton} used in this thesis; however, in-depth information will be provided in Chapter~\ref{chapter:MixedApproachesForFND}.\\
In~\ref{sec:explainableArtificialIntelligence}, we elaborate on the techniques available in explainable artificial intelligence. We discuss the qualities of a reasonable explanation and highlight the importance of the interpretability of a model. We give essential definitions that will be used throughout this thesis.

\section{Explainable Artificial Intelligence}
\label{sec:explainableArtificialIntelligence}
Understanding and interpreting a model's prediction is very important nowadays since this understanding allows us to validate the model's reasoning and extract rich information for a human expert, thus leading to to increased trust in the model~\parencite{WhyShouldITrustYou_Riberio}. In addition, explaining a model can help improve the model~\parencite{AUnifiedApproach_Lundberg} and alleviate concerns raised by Ethical AI~\parencite{MachineBias_Angwin, EURegulationsOnDecisionMaking_Goodman}.
In this section, we introduce the background for XAI techniques used in this thesis. First, in~\ref{subsec:explainableArtificialIntelligence_Foundations}, we characterize XAI by following works from~\citeauthor{TheMythosOfModelInterpretability_Lipton} (\citeyear{TheMythosOfModelInterpretability_Lipton}) and~\citeauthor{XAIConceptsTaxonomies_Arrieta} (\citeyear{XAIConceptsTaxonomies_Arrieta}) and give definitions to clarify the taxonomy. Then, in~\ref{subsec:explainableArtificialIntelligence_AGoodExplanation}, we discuss the properties of good explanations, the goals of XAI, and the evaluation techniques for explanation methods. Finally, in~\ref{subsec:explainableArtificialIntelligence_Overview} we briefly lay out the most frequently mentioned explanation methods in the literature, along with the ones we use in this thesis. We summarize each and cover explanation techniques offered to any  neural network.

\subsection{Foundations of Explainable Artificial Intelligence}
\label{subsec:explainableArtificialIntelligence_Foundations}
Initial AI methods were not sophisticated enough to require additional explanation schemes. In the last years, DNNs have been adopted in various sectors. Although empirically successful thanks to enormous parameter spaces and numerous layers, DNNs are complex \emph{black-box} models in terms of interpretability~\parencite{CanWeOpenTheBlackBoxOfAI_Castelvecchi}.\\
In the XAI context, \emph{black-box} or \emph{opaque} models are considered to be the opposite of \emph{transparent} because they require further investigation to understand their inner workings~\parencite{TheMythosOfModelInterpretability_Lipton}. Humans hesitate to use systems that are not directly interpretable and reliable, making \emph{interpretability} essential~\parencite{xAIForDesigners_Zhu}. Moreover, from a legal perspective, the notion \emph{right to explanation} brings more attention to necessity of interpretability~\parencite{TheMythosOfModelInterpretability_Lipton}. Particularly in situations such as when:
\begin{itemize}
    \item The prediction of AI directly affects human life, e.g., fully autonomous cars in traffic and medical AI assistants.
    \item The reasons behind an AI system’s decision can not be clearly determined.
\end{itemize}
With the additional demand from the Ethical AI field~\parencite{EURegulationsOnDecisionMaking_Goodman}, the research community has put in a tremendous amount of effort to gap the bridge between a black-box model and its interpretability. However, the lack of consensus on taxonomy has led to synonymous usages of interpretability. The early definitions for interpretability were too broad, describing it as an additional design driver when building a model~\parencite{TheBayesianCaseModel_Kim} or a requirement for \emph{trust}~\parencite{InteractiveAndInterpretableMLModels_Kim}.
% Nevertheless, can trust be defined objectively? Is the accuracy or F1 score of a model enough to trust the model? To answer the first question, \citeauthor{TheMythosOfModelInterpretability_Lipton} (\citeyear{TheMythosOfModelInterpretability_Lipton}) argues that trust is subjective and can not be technically defined. To answer the second question, taking only the performance metrics as a baseline for trust in the model is shown to be an incorrect approach, particularly in studies that analyze models with \emph{adversarial examples}~\parencite{DetectingAdversarilaImageExamples_Bin, AdversarialExamples_Yuan}. 
Moreover, ~\citeauthor{TowardsARigorousScienceML_Velez} (\citeyear{TowardsARigorousScienceML_Velez}) argues that the need for interpretability comes from the \emph{incompleteness} of the problem formalization.\\
Instead of finding a technical definition for interpretability, we can categorize existing systems in terms of transparency.
\citeauthor{TheMythosOfModelInterpretability_Lipton} (\citeyear{TheMythosOfModelInterpretability_Lipton}) states two properties for interpretable models: \emph{transparency} and \emph{post-hoc interpretability}. The definition for the first and its related terms are given as in the following,
\begin{definition}[\emph{Understandability}]
    “\emph{Denotes the characteristic of a model to make a human understand its function - how the model works - without any need for explaining its internal structure or the algorithmic means by which the model processes data internally.}”~\parencite{XAIConceptsTaxonomies_Arrieta}
\end{definition}
\begin{definition}[\emph{Transparency}]
    “\emph{A model is considered to be transparent if by itself it is understandable.}”~\parencite{XAIConceptsTaxonomies_Arrieta}
\end{definition}
To elaborate further, we discuss the degrees of transparent models, as not all models provide the same extent of understandability~\parencite{XAIConceptsTaxonomies_Arrieta}. Both in~\citeauthor{TheMythosOfModelInterpretability_Lipton} (\citeyear{TheMythosOfModelInterpretability_Lipton}) and \citeauthor{XAIConceptsTaxonomies_Arrieta} (\citeyear{XAIConceptsTaxonomies_Arrieta}) the categorization is made as: \emph{simulability}, \emph{decomposability}, and \emph{algorithmic transparency}. We discuss each of them briefly.
\begin{itemize}
    \item \emph{Simulability} denotes the model’s characteristic to be simulated or thought only by a human. Thus, the complexity of a model plays an important role here. Models that can be presented to a human in terms of text and visualizations are considered interpretable~\parencite{WhyShouldITrustYou_Riberio}. Simulatable models are usually elementary models~\parencite{XAIConceptsTaxonomies_Arrieta, RegressionShrinkage_Tibshirani}.
    \item \emph{Decomposability} represents the model’s characteristic to explain each part of the model. Basically, when all components of a model are simulable, that model is decomposable~\parencite{TheMythosOfModelInterpretability_Lipton}, given that the inputs are already interpretable~\parencite{XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Algorithmic transparency} deals with the user’s comprehension of the input’s journey from entering the model to becoming a prediction~\parencite{TheMythosOfModelInterpretability_Lipton, XAIConceptsTaxonomies_Arrieta}. For example, linear models can be considered algorithmically transparent since the user can understand how the model can act in a given situation~\parencite{AnIntroductionToStatisticalLearning_Gareth}.
\end{itemize}
The second property of interpretable models, \emph{post-hoc explainability}, aims to improve the interpretability of not readily interpretable models. It does so through \emph{text explanations}, \emph{visual explanations}, \emph{local explanations}, \emph{explanations by example}, \emph{explanations by simplification}, and \emph{feature relevance explanations} techniques~\parencite{TheMythosOfModelInterpretability_Lipton, XAIConceptsTaxonomies_Arrieta}. In a more general sense, post-hoc explainability methods can be grouped into three categories in terms of the knowledge of the target model (\emph{model-specific or model-agnostic}), the granularity of focus (\emph{local or global}), and form (\emph{text} or \emph{visual}). The first category refers to the explainability method's assumption of the model's structure. \emph{Model-specific} techniques can be utilized with a limited set of models since these techniques make an assumption about the model to be explained.\\
On the other hand, \emph{model-agnostic} techniques are designed not to require knowledge about the model's inner workings~\parencite{XAIConceptsTaxonomies_Arrieta, ASurveyOfMethodsForExplainingBlackBoxModels_Guidotti}. The second category denotes the explanation's domain. \emph{Local} explanations reason about a particular prediction of a model at the feature level~\parencite{TowardsARigorousScienceML_Velez} (e.g., compute a saliency map by taking the gradient of the output with respect to a given input vector~\parencite{TheMythosOfModelInterpretability_Lipton}), whereas global explanations aim to outline the model's general behavior on the dataset~\parencite{XAIConceptsTaxonomies_Arrieta, ASurveyOfMethodsForExplainingBlackBoxModels_Guidotti,TowardsARigorousScienceML_Velez}. Global explanations are usually presented in the structure of a series of rules~\parencite{InterpretableDecisionSets_Lakkaraju}. The third and last category, the form of the explanation, can be \emph{visual} or \emph{text}. We will discuss these forms of explanation in detail after we give a definition of \emph{explainability}. From now on we will talk about the explainability of a model rather than its interpretability.
\begin{definition}[Explainability]
    “\emph{Explainability is associated with the notion of explanation as an interface between humans and a decision maker that is, at the same time, both an accurate proxy of the decision maker and comprehensible to humans.}”~\parencite{XAIConceptsTaxonomies_Arrieta}
\end{definition}
\begin{itemize}
    \item \emph{Text explanations} are techniques that learn to produce textual expressions that assist user in understanding the model's outcomes~\parencite{TowardsExplainableNeuralSymbolic_Bennetot}.
    \item \emph{Visual explanations} are techniques that supplement a model's explainability by visualizing the model's behavior. Due to the mismatch between high-dimensional nature of  complex ML systems and the capacity of human reasoning~\parencite{HowTheMachineThinks_Burrell}, visual explanations often employ dimensionality reduction practices~\parencite{XAIConceptsTaxonomies_Arrieta}.
\end{itemize}
From the perspective of explainability, one would intuitively prefer transparency since transparent models
can be easily explained. However, some works argue that as the transparency of a model increases, its performance usually tends to decrease~\parencite{ExplaniableAIASurvey_Dosilovic}. Although, other works argue that this is not necessarily true, particularly in cases where the data is well structured and the quality and value of available features are outstanding~\parencite{StopExplainingBlackBoxmodels_Rudin}. Considering FND models, extensive and complex models can not be avoided since news pieces tend to be long texts, and fake news pieces are fabricated to mimic real news. Alternatively, social networks are represented as large graphs, thus forcing SOTA FND models to utilize complex approaches that decrease transparency, such as word embeddings, data fusion, and graph data structure~\parencite{UPFD_Dataset_Shu}.\\
Global explanations can be helpful to domain experts by providing information about what model has learned on a global level; however, global explanations might be difficult to obtain~\parencite{TheMythosOfModelInterpretability_Lipton}. Instead, local explanation methods are easier to obtain and more practical for real-world applications. For example, if a user requests an explanation for a prediction, local explanations can provide it, which also complies with the "right to explanation"~\parencite{EURegulationsOnDecisionMaking_Goodman}.\\
Depending on the model adopted for FND, we might be required to use model-specific or model-agnostic approaches. When dealing with DNNs in a post-hoc setting, usually, it is better to opt for a model-specific explanation as it will give a better insight into the model. However, in cases where a model-specific approach can not be employed, model-agnostic methods are the choice. Below are listed the types of post-hoc approaches~\parencite{XAIConceptsTaxonomies_Arrieta}.
\begin{itemize}
    \item \emph{Explanations by example}, a method suggested by~\citeauthor{CaseBasedExplanation_Caruana} (\citeyear{CaseBasedExplanation_Caruana}), focus on obtaining representative information from a model by providing explanations for an example that sufficiently illustrates the inner workings of the model~\parencite{HowToExplainIndividualClassificationDecisions_Baehrens, XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Explanations by simplification} techniques construct a new and simplified system to provide explanations for a model. These simplified systems keep the performance of the original model while displaying less complexity~\parencite{XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Feature relevance explanations} compute feature relevance scores for a model's variables in order to determine the effect a feature has upon a model~\parencite{XAIConceptsTaxonomies_Arrieta}.
\end{itemize}
We discussed the types of explanation methods we can adopt and how these methods can shape the design of a model. Then we laid out forms of explanations that aim to convey information about the model's behavior. It is possible to see a combination of the previously mentioned explanation forms. In order to present the user with an comprehensible explanation, we characterize a good explanation and define its important properties in the following subsection.

\subsection{What Makes A Good Explanation}
\label{subsec:explainableArtificialIntelligence_AGoodExplanation}
There is no clear definition of the characteristics of a good explanation~\parencite{XAIConceptsTaxonomies_Arrieta}. XAI draws wisdom from social and cognitive sciences to handle the subjectivity in explanations. A comprehensive study on social sciences and XAI by~\citeauthor{ExplanationInAI_Miller} (\citeyear{ExplanationInAI_Miller}), analyzes explanations in terms of the content, the \emph{explainer} and the \emph{explainee}. The author argues that the AI research lacks knowledge about the properties and structure of an explanation. The major findings from a collection of works about a good explanation's characteristics are outlined below.
\begin{itemize}
    \item Explanations are \emph{contrastive}~\parencite{ContrastiveExplanation_Lipton, XAI_BewareInmatesRunningTheAsylum_Miller}. When presented with counterfactual explanations, users understand the model's decision easier~\parencite{ExplainableAndInterpretableModels_Escalante, MLCVPatternRecognition_Lopez, MLCVPatternRecognition_Lopez, CounterfactualsInXAI_Byrne}. For example, rather than asking why event $A$ occurred, we ask why event $A$ occurred instead of event $B$~\parencite{ExplanationInAI_Miller,XAIConceptsTaxonomies_Arrieta}.
    \item Explanations are \emph{selective}. Presenting all the causes for an event to a human is pointless since humans are inclined to select a couple of leading causes out of numerous, sometimes countless, causes~\parencite{ExplainingCollaborativeFiltering_Herlocker}. Accordingly,~\citeauthor{ExplanationInAI_Miller} (\citeyear{ExplanationInAI_Miller}) argues that specific cognitive biases shape this selection process.
    \item Explanations are \emph{social} and conveyed from explainer to explainee via social interaction. Hence, explanations are transferred through the frame of the explainer's beliefs about the explainee's beliefs~\parencite{ExplanationInAI_Miller}.
    \item Probabilities may be insignificant. Even though probabilities matter when creating the explanations, the usage of these statistical relations in explanations is not as effective as that of causes. If the underlying causal explanation is not included, then the utilization of statistical generalizations is not sufficient~\parencite{ExplanationInAI_Miller}.
\end{itemize}
It should be noted that the characteristics of a good explanation are not limited to the ones mentioned above. These are the most prominent characteristics discussed by~\citeauthor{ExplanationInAI_Miller} (\citeyear{ExplanationInAI_Miller}). An important aspect is that the explanations are provided to an \emph{explainee}, also called an \emph{audience}. The audience is the person or people receving the explanation~\parencite{XAIConceptsTaxonomies_Arrieta}. It is further noted in~\parencite{XAIConceptsTaxonomies_Arrieta} that explanations depend on the audience, i.e., an explanation meant for an end-user will not be enough for a domain expert, or an explanation for a domain expert might be too complicated for an end-user. Also, we will refer to the explainee as the \emph{audience} from now on.\\
The main target audience is the primary driver when considering the needed outcomes for an explanation.~\citeauthor{XAIConceptsTaxonomies_Arrieta} (\citeyear{XAIConceptsTaxonomies_Arrieta}) summarize the pursued goals when trying to attain explainability. These goals are listed below.
\begin{itemize}
    \item \emph{Trustworthiness} deals with the assurance of a model's intended behavior when the model is presented with real-world scenarios~\parencite{TheMythosOfModelInterpretability_Lipton, StructuringDimensionsForCollaborative_Antunes}. Some studies highlight the importance of \emph{trustworthiness} as a requirement for explainability~\parencite{WhyShouldITrustYou_Riberio, InteractiveBayesianCaseModel_Kim}. The target audience for this goal are domain experts and users affected by the model decisions~\parencite{XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Confidence} refers to the robustness and stability of a model~\parencite{XAIConceptsTaxonomies_Arrieta}.~\citeauthor{Stability_Yu} (\citeyear{Stability_Yu}) argues that stability is a prerequisite when obtaining explanations from a model. Moreover,~\citeauthor{XAIConceptsTaxonomies_Arrieta} (\citeyear{XAIConceptsTaxonomies_Arrieta}) argues that an explanation method should not provide trustworthy explanations for unstable models. The audience relevant to this goal are domain experts, developers, managers, and regulatory entities.
    \item \emph{Fairness} refers to a model's potential to assure a fair prediction for a user affected by the model's prediction based on factors such as age, race, and gender~\parencite{XAIConceptsTaxonomies_Arrieta, FairnessInML_Oneto}. The audience for this goal consists of users affected by model decisions and regulatory entities.
    \item \emph{Transferability} refers to the model's capability to perform on unseen data. It is the desired goal not just for explainability but also for obtaining good performance from the model~\parencite{AppliedPredictiveModeling_Kuhn}. The audience for this goal is domain experts and data scientists.
    \item \emph{Causality} denotes the causal relationships among model variables~\parencite{Causality_Pearl}. Its primary audience is domain experts, managers, and regulatory entities~\parencite{XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Informativeness} is a goal meant for all users, and it deals with the information provided by the model. In order to fill the gap between the user's decision and the prediction of a model, a massive amount of information about the problem at hand needs to be conveyed to the end user~\parencite{XAIConceptsTaxonomies_Arrieta}.
    \item \emph{Accessibility} refers to the possibility of end users getting more involved in a model's development or improvement~\parencite{XAI_BewareInmatesRunningTheAsylum_Miller}. The audience for this goal includes product owners, managers, and users affected by model decisions.
    \item \emph{Interactivity} denotes a model's capability to interact with the user~\parencite{InteractiveAndInterpretableMLModels_Kim}. The audience consists of domain experts and users affected by model decisions.
    \item \emph{Privacy awareness} is a goal not frequently seen in the literature. It deals with the learnings of a model's internal representation such that these learnings might pose a privacy breach. From the opposite perspective, it is a differential privacy breach when an unauthorized third party can explain the inner workings of a trained model~\parencite{XAIConceptsTaxonomies_Arrieta}.
\end{itemize}
Considering a model, the evaluation metrics from the test set reflect the model's overall performance on unseen data and allow comparing different models that use the same dataset~\parencite{PMLB_Olson}. For example, metrics like accuracy, F1 score, recall, and precision are often used in the evaluation of models. Given that there are numerous metrics, it should be noted that different domains and models may require different evaluation metrics~\parencite{BeingAccurateIsNotEnough_McNee, AReviewOnEvaluationMetrics_Hossin, PeeringIntoTheBlackBoxOfAI_Handelman}.
Similar to models, explanations require evaluation methods that can quantify their performance. So far, we have seen that explanations might have different audiences, they can take several forms, and they have desired properties. Therefore, like models, there should be a set of explanation evaluation methods focusing on different categories of explanation approaches. In fact, a rigorous study by~\citeauthor{TowardsARigorousScienceML_Velez} (\citeyear{TowardsARigorousScienceML_Velez}) lays out the categorization of explanation evaluation approaches. The authors split the evaluation methodologies into three:
\begin{enumerate}
    \item \emph{Application-grounded evaluations} are conducted on domain experts interacting with explanations in a real-world application. This kind of evaluation directly tests the system's objective. Thus attaining high performance with respect to application-grounded evaluations suggests good evidence of the explanation's success. The fact that we need humans to interact with a real-world application in an environment that can be observed for experimentation makes this type of evaluation more specific and, thus, the most costly of all three types of evaluation~\parencite{TowardsARigorousScienceML_Velez}.
    \item \emph{Human-grounded evaluations} are constructed by more straightforward experiments on humans who are not necessarily domain experts. Although this type of evaluation is less specific than the application-grounded evaluations, it offers more flexibilty and is less costly. It is a good choice when the task is to test the quality of an explanation in a general sense~\parencite{TowardsARigorousScienceML_Velez}. For example, a recent study~\parencite{AHumanGroundedEvaluationBenchmark_Mohseni} used human attention maps that overlay images as explanations and asked users to rate the decision made by the model. The authors further argue that the evaluation on these attention maps can be utilized to understand the \emph{trustworthiness} of a model.
    \item \emph{Functional-grounded evaluations} do not include real humans. Instead, these evaluations use a formal definition of interpretability as a proxy to assess the explanation's quality. The lack of human dependency makes them favorable due to low cost. Typically, these evaluations are preferred, especially when conducting experiments with humans might be unethical. The challenge with these evaluations is to select the suitable proxy models. In addition, when possible, it is considered good practice to obtain verified proxies first, for instance, by human-grounded evaluations~\parencite{TowardsARigorousScienceML_Velez}.
\end{enumerate}
From high cost to low cost, and more specific to more broad, one can opt for an evaluation technique to obtain a performance indicator of an explanation. As discussed above, each approach requires a entirely different setting, which brings its shortcomings with it. Depending of the availability of resources such as time, finances, the expertise of the user, or the sufficiency of human subjects, one might have to opt for a different evaluation technique. Having highlighted essential characteristics of a good explanation by collecting insights from the literature, we now move forward to the frequently mentioned techniques used in XAI. We primarily focus on post-hoc local explanation techniques and outline their contribution to this thesis.

\subsection{Overview of Techniques in Explainable Artificial Intelligence}
\label{subsec:explainableArtificialIntelligence_Overview}
As discussed in the last section, when constructing an explanation method, one has to consider the audience,
opt between model-specific or model-agnostic, local or global explanations, and opt between various forms of explanations. In literature, there exist various combinations of previously mentioned options. For transparent models, no explanation method is needed; one can obtain relevant information in the form of weights or attention scores, given that the features are simple enough~\parencite{XAIConceptsTaxonomies_Arrieta}. In particular, we talk about explanation methods that were frequently mentioned in studies and are relevant to this thesis.\\
First, we discuss the initial methodologies aimed at gaining insight from a black-box model. The one of the initial approaches was to ask: "What happens if we remove this part of the input?". \emph{Sensitivity Analysis} (SA) deals with the analytical assessment of the effect of an omitted input variable on the uncertainty of a model~\parencite{SensitivityAndGeneralizationInNNs_Novak}. SA can be done on two levels, local and global. \emph{Local Sensitivity Analysis} (LSA) assesses the impact of the changes in the input on the output. In contrast, \emph{Global Sensitivity Analysis} (GSA) examines the effect of each variable (feature) with respect to the variations of all parameters~\parencite{InputPerturbationSensitivity_Rao}. In the literature, there are a variety of approaches for both GSA and LSA. For instance,~\citeauthor{SensitivityAndGeneralizationInNNs_Novak} (\citeyear{SensitivityAndGeneralizationInNNs_Novak}) construct a GSA procudure that employs the partial derivative of each parameter in the back-propagation algorithm to explore the change rule, which admits the \emph{Input-Perturbation-Sensitivity} (IPS) that allows for obtaining global sensitivity. An interesting example of a GSA and LSA fusion approach,~\citeauthor{SensitivityAnalysisForPNNs_Kowalski} (\citeyear{SensitivityAnalysisForPNNs_Kowalski}) utilizes LSA to reduce the number of input features and GSA to reduce the number of patterns learned by a model.\\
Another approach is to calculate relevance scores for each feature using saliency maps. The usage of saliency maps were initially adopted for image CNNs~\parencite{DeepInsideCNNs_Simonyan}, then extended to NLP in~\citeauthor{ExtractionOfSalientSentences_Denil} (\citeyear{ExtractionOfSalientSentences_Denil}) and~\citeauthor{AskTheGRU_Trapit} (\citeyear{AskTheGRU_Trapit}). Typically, salience maps compute a gradient to get a relevance score for an input feature. In other words, they convey information about the model's sensitivity based on the input.\\
A popular method used in XAI is \emph{Layer-wise Relevance Propagation} (LRP) which was first introduced for \emph{Fully Connected Networks} (FCNs) and CNNs by~\citeauthor{LRP_Lapuschkin} (\citeyear{LRP_Lapuschkin}), then extended to \emph{Recurrent Neural Networks} (RNNs) by~\citeauthor{ExplainingRNNs_Arras} (\citeyear{ExplainingRNNs_Arras}). LRP assumes that a model can be \emph{decomposed} into several layers that might contain feature-relevant information. From the last layer to the input layer, LRP computes a relevance score for each dimension of the vector at a layer. As LRP moves backward in the layers, the sum of relevance scores does not change, always staying equal to the prediction probability~\parencite{LRP_Lapuschkin}.\\
Similar to LRP, a study explaining DNNs offers another solution named \emph{Deep Learning Important FeaTures} (DeepLIFT)~\parencite{DeepLIFT_Shrikumar}. This approach addresses two shortcomings of LRP, namely, the failure to model saturation caused by activation functions and the possibility of getting misleading importance scores due to discontinuous gradients. Combining techniques from LRP and integrated gradients~\parencite{GradientsOfCounterfactuals_Sundararajan}, DeepLIFT computes importance scores based on the \emph{difference-from-reference} approach that allows the propagation of information even if the gradient is zero. Difference-from-reference is a procedure that involves determining a reference and then getting the difference between the reference and the output. This method is also later utilized to create DeepSHAP~\parencite{AUnifiedApproach_Lundberg}.\\
In contrast to model-specific approaches like LRP and DeepLIFT, \emph{Locally Interpretable Model-agnostic Explanations} (LIME), as the name suggests, is a model-agnostic method. LIME interprets the predictions of any black-box model by approximating the model around a prediction. This approximation allows for obtaining a locally faithful and interpretable version of the model~\parencite{WhyShouldITrustYou_Riberio}.\\
So far, no study unifies all the works to create one explainability framework. To address this lack of unification,~\citeauthor{AUnifiedApproach_Lundberg} (\citeyear{AUnifiedApproach_Lundberg}) offer \emph{SHapley Additive exPlanation} (SHAP) framework, in which the authors utilize recent studies from game theory based on the pioneering work of~\citeauthor{GameTheory_Shapley} (\citeyear{GameTheory_Shapley}). These studies are \emph{Shapley regression values}~\parencite{AnalysisOfRegressionInGameTheory_Lipovetsky}, \emph{Shapley sampling values}~\parencite{ExplainingPredictionModels_Strumbelj}\emph{quantitative input influence}~\parencite{AlgorithmicTransparencyViaQuantitativeInputInfluence_Datta}, and recent approaches like LIME and DeepLIFT are utilized to create a model-agnostic and model-specific explainers. SHAP values measure the feature importance and are obtained via Shapley values of the conditional expectation function of a model~\parencite{AUnifiedApproach_Lundberg}. Model-agnostic SHAP values are computed using the Shapley sampling values method, which uses an approximation of a permutation adaption of classic Shapley value estimation. Alternatively, a model-agnostic method, \emph{KernelSHAP}, employs LIME with linear explanations and Shapley values to find a weighting kernel that enables regression-based estimation of SHAP values. The authors also propose model-specific procedures, such as \emph{LinearSHAP}, which can approximate Shapley values using weights for linear models, and \emph{DeepSHAP}, which connects DeepLIFT with Shapley values. Less striking examples are \emph{low-order SHAP} and \emph{max SHAP} for model-specific explainers. We will discuss SHAP values further in Section~\ref{subsec:ExplainingNewsContentModels_SHAPFramework}.\\
In the literature, there is a lack of explanation tools for GNNs. GNNs require graphs as input and produce a prediction at a level based on the focus of the task~\parencite{DeepLearningOnGraphs_Zhang}. Graphs can represent the node feature information as well as rich relational information between nodes~\parencite{DeepLearningOnGraphs_Zhang, GNNsAReview_Zhou}. GNNs are potent tools that are designed to learn relational information between nodes and node features. This makes GNNs a perfect candidate for learning patterns from social media networks~\parencite{BeyondSigmoids_Zang}. In our case, we want to understand how a GNN behaves when classifying fake and real news pieces by explaining their propagation networks. A study by~\citeauthor{GNNExplainer_Ying} (\citeyear{GNNExplainer_Ying}) proposes a post-hoc model-agnostic approach called \emph{GNNExplainer} to explain predictions made by GNNs. \emph{GNNExplainer} takes a trained GNN, input graph(s) and its prediction(s), and it returns local explanations in the form of subgraph(s) of input graph(s) along with the most influential node features for the prediction. These subgraphs are constructed by maximizing the mutual information between the subgraph and the input graph with respect to the prediction~\parencite{GNNExplainer_Ying}.\\
Bearing in mind the FND models and explanation methods discussed, one could use LIME, DeepLIFT, or SHAP framework for news content models, which are essentially DNNs with textual data as inputs. The SHAP framework has implemented every method optimally and addressed the shortcomings of each, and provides text plots and easily interpretable importance scores, which are crucial for understanding the words or the word groups that are most important. Therefore, for explaining our news content model, we employ SHAP framework, in particular, partition explainer. Details on the partition explainer will be provided in~\ref{subsec:ExplainingNewsContentModels_SHAPFramework}. On the other hand, for GNNs, the choice is straightforward as there is only one option. GNNExplainer can be helpful in identifying the most critical spreaders of a news piece which will be discussed in Chapter~\ref{sec:ExplainingGNNs}.\\
In the next chapter, we elaborate on the news content FND model we used in this thesis. We initially introduce foundational terms for our task, we inspect Transformer models, and briefly discuss their architecture. We analyze the news content dataset and share our initial findings. We then report news content model's performance on the dataset and argue the reasons behind its performance. We construct our arguments based on the explanations we obtain from the partition explainer and share results of experiments. We stress the importance of explaining a model and understanding its inner mechanisms.
