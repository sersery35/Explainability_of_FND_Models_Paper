% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conclusion}\label{chapter:conclusion}
Automated fake news detection is becoming a necessity as more people obtain their news from social media or the web. While the source of a news piece is vital for its authenticity, history observed that even trustworthy sources could share fake news, e.g., ~\citeauthor{TheGreatMoonHoax_Foster} (\citeyear{TheGreatMoonHoax_Foster}). The dissemination of fake news leads to more misinformed people, creating a worldwide issue. Nowadays, the approaches to tackle this issue are getting more sophisticated and collaborative. For instance, Birdwatch program of Twitter~\parencite{BirdwatchOverview_Twitter} and Meta's fact-checking program~\parencite{MetaFactCheckingProgram_Meta} are actively used crowdsource-oriented approaches. The research in automated fake news detection has gained momentum. There have been sophisticated approaches which utilize social context with the news content~\parencite{UPFD_Dataset_Shu, FakeNewsDetectionUsingGeometricDeepLearning_Monti}. Nevertheless, automated FND is still challenging due to the ever-changing format of fake news, malicious accounts, outdated or missing datasets, and the lack of reproducible work. The research community should put in more effort to tackle the issue of fake news.\\
Moreover, all the FND works we have observed reported their performance for their model, but they did not attempt to utilize explanation methods to interpret the model's behavior. To our best knowledge, this is the first work that attempts to explain FND models. We strongly encourage the researchers to adopt explanation methods for their models to understand their models' behavior. As we observed in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}, we could trick the model into predicting a fake news instance as real even though the model has seen that fake news instance in training. Adversarial examples, explanation methods, and input sensitivity analysis should be incorporated into the model development process to ensure that the model will function as intended. Particularly for black-box models like DNNs or GNNs, previously stated procedures are essential. Otherwise, one cannot completely understand whether the model has learned the actual task.\\
We list our research objectives introduced in Chapter~\ref{chapter:introduction}, and we show that we have fulfilled all of them.
\begin{description}
    \item[\textbf{RO1}] Determine the tools for explaining FND models.
    \item[\textbf{RO2}] Show that explanations of FND models play an essential role in understanding the shortcomings of the FND models.
    \item[\textbf{RO3}] Investigate the understandability of explanations and potential shortcomings.
\end{description}
The first, \textbf{RO1}, is satisfied in Chapter~\ref{subsec:explainableArtificialIntelligence_Overview}, in which we discussed explanation methods for DNNs and GNNs. We provided the intuition behind the most popular explanation methods available for FND. For all models that are not graph-based, we can easily use tools from the SHAP framework; for GNNs, we utilize GNNExplainer. \\
For the second objective, we have shown first that a model can be deceived with several perturbations to the input. So we adopted the partition explainer to help us uncover this fallacy. We showed that this falsehood was closely related to the  model's architecture and the contents of the dataset. We then retrained the news content model so that we could analyze it without the features that were causing the issue. We obtained a similar performance with better base values, which suggests that the model learned to distinguish between the news by looking at more complex features. Thus, improving the model's performance and fulfilling \textbf{RO2} in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}.\\
For the last objective, \textbf{RO3}, our suggestions are twofold. The first is for the explanations from the partition explainer. We suggested the usage of bar plots of Owen values for long texts in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}. The second is for the explanations from GNNExplainer. We proposed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} to use median or mean for edge masks where the explanation is too complex. This can be further extended to apply a similar operation to the edge mask values only included in the subgraph, but we leave that for future work. We discuss the understandability and structure of explanations regarding the input data. We recommend producing explanations in the same form as the input, which would increase the understandability of the explanations. Although, this is a complicated task considering GNNExplainer attempts to explain all non-euclidean data, thus providing an explanation for every type. For this shortcoming for tree-structured datasets, we suggest aggregating the edge mask values of the edges between the same nodes so that the explanation will resemble the input. Furthermore, we suggest the adoption of interactive plots for GNNExplainer so that one can observe the changes in edges and nodes as they interact with the threshold for edge mask. We leave the implementation of this suggestion as a future work. Also, we propose to use a color palette defining the importance of edges instead of using a transparency-based approach for edge visualizations in the explaining subgraph.\\
In addition to the research objectives, we showed that utilizing social context data with news content data can increase the prediction probability by sensitivity analysis. We also discussed how to interpret explanations from GNNExplainer, analyzed its explanations' faithfulness, and observed the news content's importance for a prediction done by a GNN model.\\
We stressed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} the importance of the availability of the news content in UPFD to utilize node feature masks obtained from GNNExplainer, specifically for the root node which contains the news piece. The node feature mask of the root node can then be illustrated with the text input similar to the way the SHAP framework does. In fact, we attempted to realize this. Our attempt to construct FakeNewsNet from the authors' code took more than two months to download only around 80\% of the dataset. We also attempted to work with the downloaded part of the dataset for the explanations. Nonetheless, we were limited by the lack of specifications for the input preprocessing and the long process time of bert-as-a-service with 768 tokens maximum input sequence. Therefore, we were not able to analyze the UPFD classifier's behavior in depth, as also mentioned in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier}. Furthermore, incompatibility issues of DeepSHAP with Transformer models was another issue at the beginning. Fortunately, this limitation was mitigated by the model-agnostic partition explainer.\\
Lately, the discussion of model aging and early fake news detection has gained momentum. We discussed in~\ref{subsec:fakeNewsDetection_fakeNews} about a study by~\citeauthor{FakeNewsDetectionUsingGeometricDeepLearning_Monti} (\citeyear{FakeNewsDetectionUsingGeometricDeepLearning_Monti}). In this study, the authors show that the peak spread of a news piece happens in the first two hours. This indicates that the news pieces spread very fast for the models that only depend on the the diffusion tree of the news piece. Additionally, building the diffusion tree of a news piece in real time and feeding it to the model is a challenging issue. Hence, a model that can detect fake news pieces before they reach their peak dissemination is necessary. Otherwise, due to the psychological tendencies of humans, we will not be able to alleviate the adverse effects of fake news. The ever-changing nature of fake news content gives rise to the necessity of FND systems with continual learning, which~\citeauthor{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han} (\citeyear{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han}) discuss in detail. We encourage a similar approach for FND to create robust automated systems.\\
Undertaking these limitations should be the next step in future research. Without well-structured and informative datasets, developing a competitive model is not possible. Specifically, rich datasets like UPFD should provide more information about their process so that more insight can be gained from the dataset and explanations. We draw attention to the importance of the availability of datasets and reproducible studies and urge researchers to be more contributing on this matter. If the datasets are available and the studies are reproducible, then we can attempt to explain the model, gain insights, and suggest improvements. Without an explainable model, we can hardly tell why a model predicted a news piece as fake or real. Recall from~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier} that the Transformer model we inspected had a very good performance, however, it could be tricked into predicting fake news as real by a couple of easy perturbations. Therefore, as a final word, we emphasize the importance of explainability, and accordingly, urge researchers to adopt explanation methods when developing their models.