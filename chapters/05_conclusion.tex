% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conclusion}\label{chapter:conclusion}
Automated fake news detection (FND) is becoming necessary as more people obtain news from social media or the web. While the source of a news piece is vital for its authenticity, history observed that even trustworthy sources could share fake news, e.g.,~\citeauthor{TheGreatMoonHoax_Foster} (\citeyear{TheGreatMoonHoax_Foster}). The dissemination of fake news leads to more misinformed people, creating a worldwide issue. Nowadays, the approaches to tackle this issue are getting more sophisticated and collaborative. For instance, the Birdwatch program of Twitter~\parencite{BirdwatchOverview_Twitter} and Meta's fact-checking program~\parencite{MetaFactCheckingProgram_Meta} are actively used crowdsource-oriented approaches. For automated FND, sophisticated approaches that utilize social context with the news content have been developed, such as the study we used in this thesis by~\citeauthor{UPFD_Dataset_Shu} (\citeyear{UPFD_Dataset_Shu}) and another study by~\citeauthor{FakeNewsDetectionUsingGeometricDeepLearning_Monti} (\citeyear{FakeNewsDetectionUsingGeometricDeepLearning_Monti}) that utilizes graph convolutional networks. Nevertheless, automated FND is still challenging because of the ever-changing format of fake news and malicious accounts. Additionally, when developing automated FND systems, outdated or missing datasets and small amounts of reproducible work are often encountered problems.\\
From the perspective of explainability, all the FND works we have observed reported their performance for their model, yet, they did not attempt to utilize explanation methods to interpret the model's behavior. To our best knowledge, this is the first work that attempts to explain FND models. We strongly encourage the researchers to adopt explanation methods for their models to understand their models' behavior. As we observed in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}, we could trick the model into predicting a fake news instance as real even though the model has seen that fake news instance in training. Adversarial examples, explanation methods, and input sensitivity analysis should be incorporated into the model development process to ensure that the model will function as intended. Particularly for black-box models like DNNs or GNNs, previously stated procedures are essential. Otherwise, one cannot completely understand whether the model has learned the actual task.\\
We list our research objectives introduced in Chapter~\ref{chapter:introduction}, and we show that we have fulfilled all of them.
\begin{description}
    \item[\textbf{RO1}] Determine the tools for explaining FND models.
    \item[\textbf{RO2}] Show that explanations of FND models play an essential role in understanding the shortcomings of the FND models.
    \item[\textbf{RO3}] Investigate the understandability of explanations and potential shortcomings.
\end{description}
The first, \textbf{RO1}, is satisfied in~\ref{subsec:explainableArtificialIntelligence_Overview}, in which we discussed explanation methods for DNNs and GNNs. We provided the intuition behind the most popular explanation methods available for FND. For all models that are not graph-based, we can easily use tools from the SHAP framework; for GNNs, we utilize GNNExplainer. \\
For the second objective, we adopted the partition explainer to help us understand the news content model. We uncover that the model is basing its predictions on the existence of a group of tokens. We showed that this falsehood was closely related to the dataset's contents and illustrated the effect of model architecture on the predictions. We then retrained the news content model so that we could analyze it without the features that were causing the issue. We obtained a similar performance with better base values, which suggests that the model learned to distinguish between the news by looking at more complex features. Thus, improving the model's performance and fulfilling \textbf{RO2} in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}.\\
For the last objective, \textbf{RO3}, our suggestions are twofold. The first is for the explanations from the partition explainer. We suggested the usage of bar plots of Owen values for long texts in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}. The second is for the explanations from GNNExplainer. We proposed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} to use median or mean for the edge masks where the explanation is too complex. This can be further extended to apply a similar operation to the edge mask values only included in the subgraph, but we leave that for future work. We discuss the understandability and structure of explanations regarding the input data. We recommend producing explanations in the same form as the input, which would increase the understandability of the explanations. Although, this is a complicated task considering GNNExplainer attempts to explain all non-euclidean data. For this shortcoming for tree-structured datasets, we suggest aggregating the edge mask values of the edges between the same nodes so that the explanation will resemble the input. Furthermore, we suggest the adoption of interactive plots for GNNExplainer so that one can observe the changes in edges and nodes as they interact with the threshold for edge mask. We leave the implementation of this suggestion as future work. Also, we propose to use a color palette defining the importance of edges instead of using a transparency-based approach for edge visualizations in the explaining subgraph. This is also left for future work.\\
In addition to the research objectives, we showed that utilizing social context data with news content data can increase the prediction probability by sensitivity analysis. We also discussed how to interpret explanations from GNNExplainer, analyzed its explanations' faithfulness, and observed the news content's importance for a prediction done by a GNN model.\\
We stressed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} the importance of the availability of the news content in UPFD to utilize node feature masks obtained from GNNExplainer, specifically for the root node which contains the news piece. The node feature mask of the root node can then be illustrated with the text input, similar to the way the SHAP framework does. In fact, we attempted to realize this. Our attempt to construct FakeNewsNet from the authors' code took more than two months to download only around 80\% of the dataset. We also attempted to work with the downloaded part of the dataset for the explanations. Nonetheless, we were limited by the lack of specifications for the input preprocessing and the long process time of bert-as-a-service with 768 tokens maximum input sequence. Therefore, we were not able to analyze the UPFD classifier's behavior in depth, as also mentioned in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier}. Furthermore, the incompatibility issue of DeepSHAP with Transformer models was another issue at the beginning. Fortunately, this limitation was mitigated by the model-agnostic partition explainer.\\
Lately, the discussion of model aging and early fake news detection has gained momentum. We discussed in~\ref{subsec:fakeNewsDetection_fakeNews} a study by~\citeauthor{FakeNewsDetectionUsingGeometricDeepLearning_Monti} (\citeyear{FakeNewsDetectionUsingGeometricDeepLearning_Monti}). In this study, the authors show that the peak spread of a news piece happens in the first two hours. This indicates that the news pieces spread very fast for the models that only depend on the diffusion tree of the news piece. Additionally, building the diffusion tree of a news piece in real time and feeding it to the model is a challenging issue. Without automated fake news detection, due to the psychological tendencies of humans, we may not be able to alleviate the adverse effects of fake news on social media. The ever-changing nature of fake news content gives rise to the necessity of FND systems with continual learning, which~\citeauthor{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han} (\citeyear{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han}) discuss in detail. We encourage a similar approach for FND to create robust automated systems.\\
Without well-structured and informative datasets, developing a competitive model is a very difficult task. Specifically, rich datasets like UPFD should provide more information about their framework to offer more insight from the dataset and explanations. We draw attention to the importance of the transparency of the processes of datasets and reproducible studies and urge researchers to contribute more to this matter. If the datasets are available and the studies are reproducible, then we can explain the model, gain insights, and suggest improvements. With an explainable model, we can understand why a model predicted a news piece as fake or real. Recall from~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier} that the Transformer model we inspected had a very good performance. However, it could be tricked into predicting fake news as real by a couple of easy perturbations. Therefore, as a final word, we emphasize the importance of explainability and, accordingly, urge researchers to adopt explanation methods when developing their models.