% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Conclusion}\label{chapter:conclusion}
Automated FND is becoming a necessity as more people obtain their news from social media or the web. While the source of a news piece is vital for its authenticity, history observed that even the most trustworthy sources could share fake news. The dissemination of fake news leads to more misinformed people, creating a worldwide issue. Many researchers have tried to build FND systems in various settings to tackle this problem. With the evolution of ML systems, FND systems became automated. Nevertheless, automated FND is still challenging due to the ever-changing format of fake news, malicious accounts, outdated or missing datasets, and the lack of reproducible work. The research community should put in more effort to tackle the issue of fake news.\\
Moreover, all the FND works we have observed reported their performance for their model, but they did not attempt to utilize explanation methods to interpret the model's behavior. To our best knowledge, this is the first work that attempts to explain FND models. We strongly encourage the researchers to adopt explanation methods for their models to understand their models' behavior. As we observed in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}, we could trick the model into predicting a fake news instance as real even though the model has seen that fake news instance in training. Adversarial examples, explanation methods, and input sensitivity analysis should be incorporated into the model development process to ensure that the model will function as intended. Particularly for black-box models like DNNs or GNNs, previously stated procedures are essential. Otherwise, one cannot completely understand whether the model has learned the actual task.\\
We listed our research objectives in Chapter~\ref{chapter:introduction}, and now, we show that we have fulfilled all of them. The first, \textbf{RO1}, is satisfied in Chapter~\ref{subsec:explainableArtificialIntelligence_Overview}, in which we discussed explanation methods for DNNs and GNNs. We provided the intuition behind the most popular explanation methods available for FND. For all models that are not graph-based, we can easily use tools from the SHAP framework; for GNNs, we utilize GNNExplainer. \\
For the second objective, we have shown first that a model can be deceived with several perturbations to the input. So we adopted the partition explainer to help us uncover this fallacy. We showed that this falsehood was closely related to the  model's architecture and the contents of the dataset. We then retrained the news content model so that we could analyze it without the features that were causing the issue. We obtained a similar performance with better base values, which suggests that the model learned to distinguish between the news by looking at more complex features. Thus, improving the model's performance and fulfilling \textbf{RO2} in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}.\\
For the last objective, \textbf{RO3}, our suggestions are twofold. The first is for the explanations from the partition explainer. We suggested the usage of bar plots of Owen values for long texts in~\ref{subsec:ExplainingNewsContentModels_ExplainingNewsContentClassifier}. The second is for the explanations from GNNExplainer. We proposed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} to use median or mean for edge masks where the explanation is too complex. This can be further extended to apply a similar operation to the edge mask values only included in the subgraph, but we leave that for future work. We discuss the understandability and structure of explanations regarding the input data. We recommend producing explanations in the same form as the input, which would increase the understandability of the explanations. Although, this is a complicated task considering GNNExplainer attempts to explain all non-euclidean data, thus providing an explanation for every type. For this shortcoming for tree-structured datasets, we suggest aggregating the edge mask values of the edges between the same nodes so that the explanation will resemble the input. Furthermore, we suggest the adoption of interactive plots for GNNExplainer so that one can observe the changes in edges and nodes as they interact with the threshold.\\
In addition to the research objectives, we showed that utilizing social context data with news content data can increase performance by sensitivity analysis. We also discussed how to interpret explanations from GNNExplainer, analyzed its explanations' faithfulness, and observed the news content's importance for a prediction done by a GNN model. We stressed in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier} the importance of the availability of the news content in UPFD to utilize node feature masks, which can then be illustrated with the text input similar to the way the SHAP framework does. Thus, we could not analyze the importance per token in a news piece because of the limitations we enumerated in~\ref{subsec:ExplaniningGNNs_ExplainingUPFDClassifier}.\\
Furthermore, incompatibility issues of DeepSHAP with Transformer models, outdated datasets, and the availability of the datasets were other limitations. For instance, our attempt to construct FakeNewsNet from the authors' code took more than two months to download only around 80\% of the dataset. Therefore, we were not able to analyze the UPFD classifier's behavior in depth. Fortunately, the incompatibility issue of DeepSHAP was mitigated by the model-agnostic partition explainer.\\
The research community took a particular interest in FND, and new studies are considering these limitations in their designs. Lately, the discussion of model aging and early fake news detection has gained momentum. As we discussed in ~\ref{subsec:fakeNewsDetection_fakeNews}, fake news tends to spread too fast. Hence, a model that can detect fake news pieces before they reach their peak dissemination is necessary. Otherwise, due to the psychologica tendencies of humans, we will not be able to alleviate the adverse effects of fake news. The ever-changing nature of fake news content gives rise to the necessity of FND systems with continual learning, which~\citeauthor{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han} (\citeyear{GraphNeuralNetworksWithContinualLearningFakeNewsDetection_Han}) discuss. We encourage a similar approach for FND to create robust automated systems.\\
Undertaking these limitations should be the next step in future research. Without well-structured and informative datasets, developing a competitive model is not possible. Specifically, rich datasets like UPFD should provide more information about their process so that more insight can be gained from the dataset and explanations. Therefore, as a final word, we draw attention to the importance of the availability of datasets and reproducible studies and urge researchers to be more contributing on this matter.